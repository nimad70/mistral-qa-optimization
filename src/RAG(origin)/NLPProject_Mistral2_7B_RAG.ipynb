{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5_x9N13AKyiu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0d7f2af4-89e3-4b70-84a7-bc8132829006"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for FlagEmbedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["%pip -q install git+https://github.com/huggingface/transformers\n","%pip -q install -U datasets sentence-transformers loralib sentencepiece bitsandbytes accelerate einops  huggingface_hub\n","%pip -q install -U pypdf chromadb PyPDF2 langchain FlagEmbedding InstructorEmbedding langchain-community"]},{"cell_type":"markdown","metadata":{"id":"ndxOH6MI6APt"},"source":["###HuggingFace Login"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hebEJuaGT_Fb"},"outputs":[],"source":["from google.colab import userdata\n","\n","userdata.get('Polyjuiceai')"]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"id":"2a0JWlNdK85m","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["83be7aa6f3344d81b8ad8c4e5ad7e136","2b8fc2e50d804cba8eb3368775eb50a0","e13d180bac5445b8858cf5c80ce0f718","4058760247e74907a04b8729389d6328","52d5e6bf4dc0490daa370ccd992000c8","75d53e90c9814be0aeee54d75d89a918","0311b7aeec7842b49eb435175fdda52e","309bf044781f4a10b91a6e798404a2b1","76907780c959474ea1454479fdf1817e","df8924d3eeb9443987b5bb4a0e337559","d7c3cffa16724dc4b6c52286b0934be7","f0e15ef9bd064c07a3bc953ce33c01c3","450cd70095d04eea8517c5fe166c67e0","63402e4d5703412b8316bd60b2e5757d","2cc5daf7a2af4d8ba7314db1ebf81b8b","524e828f7231495aa6ee3afb121290db","cfde14930ec24b608202eb3cc7595f9c","3c8fa1e78df34b25a6308c6b5bdc26af","e11b6624a580435bb7127b36035ad4a3","3e13dd3187504ee49be7ffe39ddf4caa","3c4990652e534814945014d27f2e89ea","41dea8cd57724383bd7d40cd9bfcc564","a8ba9fe549734470b0ce991158d23730","abe1515d56b441eb95decce85907fb88","701ab428cb684773a4f966c722490cdf","3daeae51af814438913dc81c9d3247de","055acc58e5314a6a9e5f4cc032baf737","d000e9b998f548069b17106341dea55d","ec954ed541574533a83364ca0bbac30c","1cf662e808824297909f2ce419d86148","a0b1aad197aa492894a9411323c50dab","3bc4d72416fe427796684173688c805d"]},"outputId":"0c7302ae-ade9-4453-b68f-0f189bbaa30c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83be7aa6f3344d81b8ad8c4e5ad7e136"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"cpC_r9cT6E74"},"source":["### Frameworks/Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eRswMcgcT_JP"},"outputs":[],"source":["# HuggingFace\n","import torch\n","import transformers\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForCausalLM\n","from transformers import pipeline\n","\n","# LangChain\n","from langchain import HuggingFacePipeline\n","from langchain import PromptTemplate\n","from langchain import LLMChain\n","from langchain.vectorstores import Chroma\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.chains import RetrievalQA\n","from langchain.document_loaders import TextLoader\n","from langchain.document_loaders import PyPDFLoader\n","from langchain.document_loaders import DirectoryLoader\n","from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n","\n","# embeddings\n","from langchain.embeddings import HuggingFaceBgeEmbeddings\n","from InstructorEmbedding import INSTRUCTOR\n","from langchain.embeddings import HuggingFaceInstructEmbeddings\n","from sentence_transformers import SentenceTransformer\n","from sentence_transformers.util import cos_sim\n","from sentence_transformers.quantization import quantize_embeddings\n","\n","# to create chain\n","from langchain.schema import prompt\n","\n","# formatting the response\n","import textwrap"]},{"cell_type":"markdown","metadata":{"id":"O74ftbYf6MKt"},"source":["### Loading Model, and its Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":612,"referenced_widgets":["06706cb58b0e45308a8b9308da3ad9e8","3a8a355c1de54d70bc02db03f3d143de","5eb2c25d8e0e4191a218577a50eb8b0e","9472445b06434513b1cfba31cdea6d65","c55efce0462a4d43a221b90776bb4cfb","69655b69b90040629033ea37da54369d","f5995cd08cf24890ba696057de05aa74","601d1aadd78d444ab6b1cf59eab557db","4ff4f6ec0290451b9258fc085ead19c5","0ab3eb5f6a314e38a7299a1e5f07b9a6","295e0e7f15ec4d1a8c3a56a66025d3ee","a8fcd1f42ef64588835e88075527292b","05c659300e054498ba341fae2afb98bd","8c4573c889fc4655b464e6a269c155a2","914322c4fa54422b8502cd51b6f9c4f0","ba57826641444374a50f5a45039b922e","b8211887d9cc41d88787f28175592b5c","d51559af1d8a47428140513eb483b8da","50ae5c800a9d4f78856cece638eb40e8","52b6e771814e4b5ca97ca7925d70e13a","70f2dc2066384d62948d4c220a7c900b","491d954628214bad91dde80ca8c0d8a0","6f3f73c5b06648c6945c81915c6e9308","ed026d8c163d49e6a89a76efa763dfb7","9a41777a1d7e4ac78aa5fcdcf38e253e","0f5624bf2d2b498290aa2d7ec014e342","ef6095bb224c47488a741cc8a7940f8f","a6b68f6fa9f34a9cb75ee12396143052","b126a40185c64757b71c1492912ce9e2","dc49e24b62154e43a91311826530cfaf","b52723d8649240e7ac4e5d339200e6c0","259b7322eaca44e387574cf366bedfca","0b96ddd6145b48189760119f5c6b4632","0cef02e719a04fa2ba3d3a82a39a0a5e","786b638d187b41fca8af1d8932333878","ac77addcaaeb406da998f1066135937a","91b690de32af47baa5660bf0fc7bae90","02323dbb179c4431a4c4ba6314199cb0","acb34be0f8ab46a78f03775337c4e9db","bafbf758b9864d28aac252fc635c8ce4","06e90fd0f23342d7b148b144e0a54809","876dea86836c4d5d96f09a3ba7e343c5","a562d13bacca4494a307b0c655f3943b","5f92206b6dd3488c9796254e8b2bac7d","ce4e0277f9c9498c872f0549e8cfb0e1","06c135caef9642de9e322a05a9376798","103dd651f1524c23b1e0b9cca25846d8","af2fddb2d5f74515b3d095e2f9341433","f03961e9cc5a4e198edd35dbffeffe9d","b9a3d23179a545998a10a42a99dc9c39","7cd8a7abbbdc4670a052db70c37596d2","56934cde1bd44788844657fe5801f6e0","eb97f541a68f48278204fb2a285a092d","0592760a6d9a47d9b8c92884a8245f32","b6203695efdc40fb82d82301bf03d564","e3fa7e54443b47a2b62a83128b98d990","cf4f4c8473b44f3ab5b7ca7f5361e89d","c1455e5a37594a7b965374dabe7f3e04","f8c45a0102c843589e9d13f5de79c0bc","881504ac870a4545b81cffabf46a127d","53ab111c9e5d4541a77751fe2cad6524","c1caf176f8b84301967f164b373adf5a","df2cdb18047547d48c83172ec3bd89bd","e9bf614a16ec4332b774e0a163d00fcb","4d058d8670044e118e54cc8acc9414da","06fdb6bd0acb4b35a2c7c6809d4834d8","4cbb6dec2a2740e6b387b4f71618156a","47ee4856f118412caab8f940f1f0ae3f","24a7ff75918e4e23bf107328e28e9f52","220b68131e304a7b979df51fd41aa35f","1f495fd897fc4b85bbf8aa90d9dc11b8","0899e1400f184c519cf16d43192477d8","a10bfddfdbd3489e9d381306487994d0","038d4d615f8b4dcab84b987d274c6a9c","eee0ee55326245b6812da534a369ecfb","4a130bf9aae5470586a0c3eaba589f27","0ce6c558e19f432f90b8055ba2d12fb4","2e49553d53614a18a365e3f7dcab9b31","98717ece4417433d9b623182a50eeb94","2582ba3c3fc745428a1cb1e836b6d8ae","b6645a693a2a4776bda2cb80a022195d","956b0ed0794141b59c2483db257d3237","6ed3a1d974ff41d2b0da4300674d4ae4","8983b8243d514e3ba30e468eff624a8a","44b75b1ef0e44e83b66496dd0b8697ff","21e5f711e0e04cf6ae04ba75729c87bf","68328d064a774b2bad4c8fba8c63d9cb","7c49beb9e44e4406a9a94e29685b7602","90eee8f51b1e49f791f380268b155b1a","6c99e52b564f4f7d9792ec89ac3d711e","a3a8ed9e1d2d42a5b9db1f8f253f2fc6","cfc4b9823f5d4ce880c8b1385ee94939","552c4ffe362d4d4290cfb5b0b2054a85","77b9806b57ad408691f05e6fd367a3fd","74378a405fc647b293ef2ffe5b0737ac","8800ac3e2db4412cbde6b23e53f5a143","ac7c98b9aa444f03bfc4d00cfa0f534d","f749d881390448aa826b1b489609bdb7","78168884f61f4ac9bf7f83a5f07f0f09","298ace6d5e63469895288e4084b006ec","c28df59f6a944a29be3251bd867e4b17","87efa7dc0f904ddd8d76dbea743736e3","5c3419023ecf4214867cf8cf67babb06","10c6088f22ad412e968ce1aea13e847e","190cdb0b0e09408b8dfbd5648830e5e7","77f9e5643ec2496f9580e360099e582d","de4a409116254463acce09b9ec9dec95","1f29660e99864f53bd11610771b7d10a","521f42d4bc0c4a9ca525db552bf07e81","28bba7700b6b4aee88c8ad1647d6437d","5278baa3cc09475f9bb07b4d41d57728","83863777f6ec412b9f8639914694efd7","ba46dd93b4ac454b884fea8b0ca34523","c8a4a5b924f84540b3ab57c95db495af","1b53b8d75669428786559079914b3132","8eb1cbdba6204371a10a0882aff8b68a","41e3eac9368546f8953f147962936278","c33940b6b2694808b9ec5b77aa755337","c67df4dd9a5e4e3a948c6b1442bfe715","42a3f7a7ff1a494192966ae0d18d176e","c361b7cd5d164018b62c977b6e991269","71d9c9084c7c488e8e8054948dd175af","64d88a5f7f7940d6a23e8a3311469cd5","bb1146d51a104a33aa42148972616bd4","f09357a79e3a4789942d5edad77ab3d9","ce4ae1bf08f2471496e628257ebb817b","b4832976a9f34a1085e3203af5635309","b4a562e3421945f1b5a7772b921dcf04","bfd2732821d84460aa128b456ed25998","0549289a02ad4678a4559b358927c029","5a740d4a884541bda0e0be69070ef636","ca42d55ff1794cb495be455b8658963f"]},"id":"IhXWIiuvT_M_","outputId":"1c333092-dcd7-4b6b-e863-3e4693a2c522"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:769: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06706cb58b0e45308a8b9308da3ad9e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8fcd1f42ef64588835e88075527292b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f3f73c5b06648c6945c81915c6e9308"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cef02e719a04fa2ba3d3a82a39a0a5e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce4e0277f9c9498c872f0549e8cfb0e1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3fa7e54443b47a2b62a83128b98d990"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cbb6dec2a2740e6b387b4f71618156a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e49553d53614a18a365e3f7dcab9b31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90eee8f51b1e49f791f380268b155b1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"298ace6d5e63469895288e4084b006ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5278baa3cc09475f9bb07b4d41d57728"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71d9c9084c7c488e8e8054948dd175af"}},"metadata":{}}],"source":["base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name,\n","                                          use_auth_token = True)\n","\n","model = AutoModelForCausalLM.from_pretrained(base_model_name,\n","                                             device_map = 'auto',\n","                                             torch_dtype = torch.float16,\n","                                             use_auth_token = True,\n","                                             load_in_4bit=True # 8bit/4bit\n","                                             )"]},{"cell_type":"markdown","metadata":{"id":"htViqv4RV_5J"},"source":["###Building Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2HNTY9vT_QQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"464b9dcd-9350-44c6-c51c-9f3e793584ac"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n","  warn_deprecated(\n"]}],"source":["# To create a text generation pipeline\n","pipe = pipeline(\"text-generation\", # specify the task for the pipeline\n","                model = model,\n","                tokenizer = tokenizer,\n","                torch_dtype = torch.bfloat16, # data type for PyTorch tensors\n","                max_length=2048,\n","                temperature=0.1,\n","                top_p=0.6,\n","                repetition_penalty=1.15,\n","                max_new_tokens=1024,\n","                device_map = 'auto',\n","                do_sample = False,\n","                top_k = 50,\n","                num_return_sequences = 1, # only one text sequence should be return for each input\n","                eos_token_id = tokenizer.eos_token_id,\n","                pad_token_id = tokenizer.eos_token_id,\n","               )\n","hf_llm = HuggingFacePipeline(pipeline=pipe)"]},{"cell_type":"markdown","metadata":{"id":"UuLgUewuJjVP"},"source":["###Setting up Langchain to retrieve PDFs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hibbLkK-Jigw","outputId":"9f4083d7-d4bb-42ce-9551-e3590ae9bc6d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["171"]},"metadata":{},"execution_count":9}],"source":["# Load and process a single text files\n","loader = DirectoryLoader('/content/papers', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n","documents = loader.load()\n","len(documents)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D2bpEn5yKUlG","outputId":"67a816ed-fe40-4458-9de0-4d25a2615073"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["992"]},"metadata":{},"execution_count":10}],"source":["# splitting the text into chunks\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","texts = text_splitter.split_documents(documents)\n","len(texts)"]},{"cell_type":"markdown","metadata":{"id":"nh-wOyQIcnK4"},"source":["###Text Embeddings\n","\n","MTEB is a massive benchmark for measuring the performance of text embedding models on diverse embedding tasks\n","\n","https://huggingface.co/spaces/mteb/leaderboard"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369,"referenced_widgets":["aa7f8598cdc8436b854d54440f74307e","9b6ce4e3070941b3b02a6ed6681ba349","b9b085c19f814d5abc51e6bd693145d5","2a135b5f305e43d480d6bb74d48cdeca","475544bad9f24e81bcb1493f054f64e5","db3aaba22c884e388716d7a193101d80","e7266f328fff4e0c8b1c99c2731a6286","e0178535c33540f89a9fd34cf0642756","78c6509ffa484f77ac67fcafbca02f2a","c1964b39c8684b07acc060097fbf406f","f613a5f8fb64492b880ce26d1da522ae","08d23fca9fd9460a927f890f81894199","c0afc32da76a4e94a437ab6ab685b0b8","fb8242d10a9845658d85dc87ec30eac1","8fa10a987e854b97831d104bf10301b3","895c67c123534d5eb6b3c00c8ef1d251","c498d2fac37b4649a9f5e1623ba78f55","43371bb779ca4edbb30a431d3f94381d","c097233e770945339b41dde3a5ad88fc","36d8f43ea9c147059ffa64a59f062a82","d12e0ebb763c49a09d82dd3fbcb7a5ff","f22a70183b0643bfa9c9fe6633856f5a","29fdc46971ea4d1fbcb72f3e1866259e","46b6fe67aebc45808f5908088912d6ef","0b3c8d11eb17401e8d98229ba3bf5206","3f4eb0fc48574db4b5bf090207097a15","7bae81156a2342eca11c4ad99fdaeaa2","d25c6f2e5da745afa38e87016ae88341","53234aaa710e483ca36f5b43d1cb093f","582e1f8f0bab45e590f01e75231b6ee5","274b0f09966d4bc0b054173c1b4865de","7301dcbc8ff342389bf4bd907a11320e","c842aeac41394a42b7a175e2e1e11d43","d18426fe6afb4530b90ab3610c55e46c","c4f2fff67c07478387c5621aa76d5a01","57852ec2b1bb4d8d90b9746f69296a09","821e4f9dc3d645de9c0dbed50bda2c19","6eae54b8242d4c2fbc3fc8f0239f9518","3bb8f0d30f7b42c4848eca00649d485a","887ff7b3de844b5bb06aff45184a746a","276efaaf44d24bac93bc649beeb9cd1d","092077b04cbb48dfbb37f616945d8e64","0e45fb8f43d74801a00edece3702262e","74d540cf785944fdbfc631ba25b911ee","5d04aca9e30f4c6e8a9cd6883e13955f","f25b47e90fdd460587d030c912716604","a217c3b988b64c79a4414ae864761082","04080a985fa34db29af8994f6a0bc194","0475fb1047c8432e8390fe5313583a94","1d01e9a266b440cb95424dcee3ed86cd","62e33de9223c4ea1b8da81e211a359eb","573e2e0409d340a68134f7f9039ca982","847e3d06d735443eb3766ec2c5d08c33","b91e3640d2ba43ea9f05a9d0effb5e97","6c122c2b2d9540f194738e19c26b8829","a9256d9eff364f7db99f28071beaec8f","675920c17bfa47eab84156c93e3233c2","fcf879b0ffa24d9e9af715fefbaddb30","74f5d441552842e19b2f57ee863353fb","59189e86ae8847ce8a9188d8b5f5f306","d92ad649906b4e5ca18b7f05bcdbb34a","8894308273814e949bf2d16aed220b7a","459aa63b5d194fb2876f42ad75193596","9f4ade787d05442fb5eff0c0313823d6","5e7e2055d44c4c7095842d2dda9b4d70","27887d02f20b4af387015038c80227cd","aa289955cbab4a2b8b5ec4c4d067a9b4","a90e4d363d24442a98d380d6631eda38","85e940703c614b749d03bb44056d02fd","fa5997b87c2d401ca9f69b28c5d7df87","8bc21f36cedd4fa7bc6f512cf8ffa3a0","86686f9edc7c4c5c879679eb627932b4","de00049777574399925d8e71b70cabc3","a1a0d8760baf4bcb8827dd34aa671da9","0ac13195f33f46e283ed2e28b5e72ebc","a5677316e59a43d8aa06d361247d9885","85476ce4f11042adbb52ee73d6fe9c69","ebd1a35fb0244a08a32547a7d973f0ba","e8a5681f3e494090b8baafbfd40f2a48","68559088e7fb4570ac2c7d024aa55616","5169342dfd984465870058f82c17caa6","7d12f90c93f64aa98b35159be41df31d","50b68962b3cd4e1a88a609b88c70f07a","dc14a89bb78b49b0b7295b4049771ac9","6a305d29d982415b9a932f0d95049d7b","1a8639d5319244629e9355e99216870a","4db51d77cc4b4004945958382a50210f","eb06afcaf0a048acb53215feb3b8152a","6bb99b9b8c2646e7a2ae77bc17c46786","e56ebfae77ea446f9ec831df0948f28a","e0ed8ea4b1814a84aa3611a61d546a12","d6888c5e6ba2462582e2d0509fe953e5","d708d2bface9447e87da397af97dc636","f08dcdd36b34479ab38fdf41b5e1e07c","ce9302b49e7a4ee1bdcacef65905c336","4d246c45da114a56b2f100c09ac8de33","f6f3d5df732b4794986646e5316e2be1","eeed601a7a40419296cca362b590199c","40b32373146f44e4a5603fba2f80e37d","80a7b39e038c428993c73265fece8618","a3b89c40955e47efb0565c2e1b85f351","bf426203b4e14ef9b9489745c74a2f58","444f60b2d11f462e91233cc502fc015b","23001c77a6f247beb4f1546bbec6bd91","c8be0ac1805f4547ad692f72cdb67051","cf393ce8e37046f380d26b27b131e1bb","c9495edc23934e3c8be307e95c66c629","6e4822bc267e430780ef8f8262063df5","c1845eb536b3491d9824cf20828cd842","187f09c8b1a142908c698d97325f4a77","30509761bd574ad3a8f1591d12ffff56","447c8d3d6c4644d0a16cc721aaeff4d3","0bec6eebe15e43498aad7f81b8b8035b","a6e5a6c48fab4d008f19169caf3b94d7","b2263caa7c5a42a481fe81797be054f2","a05ee481e54d41eb9fe275f94bcee303","eb610cc546dc43eb81b32a2894183c8f","9d3219cfb79d48ff958a05ce55cf6dbb","ac4549aaccbe4ca3954c2e72087f715b","7939b8293bb94ddcb795462e138b605d","18ff9af2289e49418f3e557f7fe19ac6"]},"id":"7rN6-VsBKwir","outputId":"428d57e5-6463-40ef-d6a4-7ff97a6252be"},"outputs":[{"output_type":"display_data","data":{"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa7f8598cdc8436b854d54440f74307e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08d23fca9fd9460a927f890f81894199"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29fdc46971ea4d1fbcb72f3e1866259e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d18426fe6afb4530b90ab3610c55e46c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d04aca9e30f4c6e8a9cd6883e13955f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9256d9eff364f7db99f28071beaec8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa289955cbab4a2b8b5ec4c4d067a9b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebd1a35fb0244a08a32547a7d973f0ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bb99b9b8c2646e7a2ae77bc17c46786"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80a7b39e038c428993c73265fece8618"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30509761bd574ad3a8f1591d12ffff56"}},"metadata":{}}],"source":["embedding_model_name = \"BAAI/bge-base-en-v1.5\"\n","encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n","\n","model_norm = HuggingFaceBgeEmbeddings(\n","    model_name=embedding_model_name,\n","    model_kwargs={'device': 'cuda'},\n","    encode_kwargs=encode_kwargs\n",")"]},{"cell_type":"markdown","metadata":{"id":"jmj7ETdwBqHW"},"source":["###Chroma DB\n","\n","utilized for the storage and retrieval of vector embeddings.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3pbPv_xLn7O"},"outputs":[],"source":["# Supplying a persist_directory will store the embeddings on disk\n","persist_directory = 'database'\n","\n","# embedding = instructor_embeddings\n","embedding = model_norm\n","\n","vectordb = Chroma.from_documents(\n","    documents=texts,\n","    embedding=embedding,\n","    persist_directory=persist_directory)"]},{"cell_type":"markdown","metadata":{"id":"-HciOUGFctmZ"},"source":["###Retriever\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6T0zABxycsNU"},"outputs":[],"source":["# search_kwargs: to return the number of context from the vector stores\n","# also we are having to have a citation to show from which pdf file the info comes from\n","retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"]},{"cell_type":"markdown","metadata":{"id":"zb0Sn22Ke6kT"},"source":["###LangChain\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S64lItNBe8XP"},"outputs":[],"source":["B_TOKEN, E_TOKEN = \"<s>\", \"</s>\"\n","INSTRUCT = \"### Instruction:\\n\"\n","QUESTION = \"\\n\\n### question:\\n\"\n","RESPONSE = \"\\n\\n### Response:\\n\"\n","\n","DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n","You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\"\"\"\n","\n","\n","# Creates a complete prompt\n","def create_prompt(user_query, system_prompt=DEFAULT_SYSTEM_PROMPT):\n","  prompt_template = B_TOKEN + INSTRUCT + system_prompt + QUESTION + user_query + RESPONSE + E_TOKEN\n","  return prompt_template\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f15ov0aYgsmx"},"outputs":[],"source":["# diffrent system propmt\n","system_prompt = \"\"\"You are a knowledgeable, helpful, respectful, and honest assistant specializing in medical expertise and artificial intelligence. Always provide thorough and accurate information, ensuring your answers are as detailed and helpful as possible while maintaining safety and ethical standards. Your answers should avoid harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Ensure that your responses are socially unbiased, positive in nature, and supported by relevant knowledge or research. When appropriate, provide examples, explanations, and context to enhance understanding. Always answer using the context text provided. Your answers should only answer the question once and not have any text after the answer is done. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \\\n","The generated responses must be accurate, precise, coherent, and consistent with the questions. They should be brief and not too long. For more advanced questions, provide a very short background and a concise summary while giving enough details. The next criterion in double asterisks is very important to follow: **Do not exceed the maximum length of the response. The maximum length of the generated response is defined as 512 new tokens; try not to exceed this and give a complete answer within this limit. Ensure that sentences are complete and ideas are fully conveyed within this constraint. If the response is too long, ensure to summarize the key points effectively while maintaining clarity and completeness. Always prioritize brevity without sacrificing essential details.**\n","\"\"\"\n","\n","instruction = \"\"\"CONTEXT:/n/n {context}/n\n","\n","Question: {question}\"\"\""]},{"cell_type":"markdown","metadata":{"id":"HKlDR9wznrAs"},"source":["### RetrievalQA\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4bvRKJmnVaC"},"outputs":[],"source":["# Create the template prompt\n","prompt_template = create_prompt(instruction, system_prompt)\n","llm_prompt = PromptTemplate(\n","    template = prompt_template, input_variables=[\"context\", \"question\"]\n","    )\n","\n","chain_type_kwargs = {\"prompt\": llm_prompt}\n","\n","# To create the chain to answer questions\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm = hf_llm,\n","    chain_type = \"stuff\", #  uses all of the text from the documents in the prompt\n","    retriever = retriever,\n","    chain_type_kwargs = chain_type_kwargs,\n","    return_source_documents = True\n",")"]},{"cell_type":"markdown","metadata":{"id":"KVmX_XdRWsdb"},"source":["###Format the generated response"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TD8aIxbRrnuL"},"outputs":[],"source":["# To trim a given text\n","def trim_text(output_text, search_phrase):\n","  phrase = search_phrase\n","  index = output_text.find(phrase)\n","  if index != -1:\n","    return output_text[index:]\n","  else:\n","    return output_text\n","\n","\n","# Removes occurrences of a substring from a text\n","def remove_substring(output, substring):\n","  return output.replace(substring, \"\")\n","\n","\n","def wrap_text(text, width=150):\n","    # Split the input text into lines based on newline characters\n","    lines = text.split('\\n')\n","    # Wrap each line individually\n","    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n","    # Join the wrapped lines back together using newline characters\n","    wrapped_text = '\\n'.join(wrapped_lines)\n","\n","    return wrapped_text\n","\n","\n","def process_generated_response(generated_response):\n","    wrapped_response = wrap_text(generated_response['result'])\n","    final_response = trim_text(wrapped_response, '</s>')\n","    final_response = remove_substring(final_response, '</s>')\n","\n","    print('\\n\\nSources:')\n","    for source in generated_response[\"source_documents\"]:\n","      print(f\"{source.metadata['source']}\")\n","\n","    print()\n","\n","    return final_response"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qfUWg3Iv4OjA","outputId":"f7e0abab-dbd8-4ba1-8314-9705d117a9c0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/JMLR.pdf\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","/content/papers/2403.13600v1.pdf\n","\n"," Great question! Large Language Models (LLMs) refer to artificial intelligence models designed to process and generate human-like text based on\n","input data. These models can learn patterns from vast amounts of text data and use that knowledge to perform various natural language tasks such as\n","translation, summarization, and text generation. Notably, recent advancements involve fine-tuning these models for specific domains like medicine or\n","education, improving their performance in those areas. Some well-known LLMs include BERT, RoBERTa, and T5. These models have been trained on massive\n","datasets, enabling them to understand long-context dependencies and generate coherent and contextually appropriate responses.\n"]}],"source":["query = \"what are Large Language Models?\"\n","response = qa_chain(query)\n","print(process_generated_response(response))"]},{"cell_type":"markdown","metadata":{"id":"uRfwvXgu2wo6"},"source":["# Retrieving questions and generating responses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwLYCDl7jnw5"},"outputs":[],"source":["import pandas as pd\n","\n","\n","def create_question_dict(questions_file_path):\n","  qfile_path = questions_file_path\n","  dfQ = pd.read_csv(qfile_path)\n","  dfQ.dropna()\n","  qlist = [dfQ.loc[i, 'Question'] for i in range(len(dfQ)) ]\n","  qa_dict = {key: None for key in qlist}\n","\n","  return qa_dict\n","\n","\n","def generate_qa_dict(question_dict):\n","  qdict = question_dict\n","  for k in qdict.keys():\n","    query = str(k)\n","    response = qa_chain(query)\n","    final_res = process_generated_response(response)\n","    qdict.update({k : final_res})\n","\n","    return qdict\n"]},{"cell_type":"markdown","metadata":{"id":"0JqYaxt8BMO4"},"source":["###General QA"]},{"cell_type":"code","source":["file_path = str(input(\"Enter the file path: \"))"],"metadata":{"id":"rbLaqlFHSDtk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c9ce3fc4-4547-46b3-ae0d-42d93453ba64"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter the file path: /content/HR_QA.csv\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IPCcoSyC4XLd","outputId":"8a9fb6a3-0b09-4920-ce3c-5b5fddec832a","collapsed":true},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/2403.15360v2.pdf\n","/content/papers/2403.15360v2.pdf\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/awad411.pdf\n","/content/papers/awad411.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/JMLR.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/JMLR.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/euae110.pdf\n","/content/papers/JMLR.pdf\n","/content/papers/euae110.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/awad411.pdf\n","/content/papers/awad411.pdf\n","/content/papers/awad411.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/awad411.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/euae110.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","/content/papers/s12276-024-01233-y.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","/content/papers/euae110.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","\n","\n","\n","Sources:\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","\n"]}],"source":["# General Questions\n","\n","questions_file_path = file_path\n","Q_dictionary = create_question_dict(questions_file_path)\n","\n","for k in Q_dictionary.keys():\n","  query = str(k)\n","  response = qa_chain(query)\n","  final_res = process_generated_response(response)\n","  Q_dictionary.update({k : final_res})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ka9sG8ZUMnOC","outputId":"d3fddf95-4b35-4799-f26c-5ef8730a14e3","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Q0: Can you explain the difference between supervised and unsupervised learning in the context of AI?\n"," response:  In supervised learning, an AI model learns from labeled data, which means each input example comes with a corresponding output label. This\n","relationship allows the model to predict outputs for new inputs based on the learned patterns from the labeled data. Examples include image\n","classification, where images are labeled with their respective classes, and regression problems, where continuous values are assigned to inputs.\n","\n","On the other hand, unsupervised learning involves processing unlabeled data, meaning there's no explicit correspondence between inputs and outputs.\n","Instead, the model identifies inherent structures and relationships within the data, often through techniques like clustering, dimensionality\n","reduction, or density estimation. Unsupervised learning can reveal hidden patterns and features in complex datasets, making it useful for applications\n","like anomaly detection, recommendation systems, and feature extraction.\n","\n","In the given context, the authors compare the performance of the SiMBA architecture on both supervised (transfer learning benchmarks) and potentially\n","unsupervised (not explicitly stated but could infer from the description of the SiMBA block process) tasks.\n","\n","\n","Q1: What are some common applications of AI in the medical field today, and how do they improve patient care?\n"," response:  Great question! AI has become increasingly prevalent in various aspects of modern healthcare. Some common applications include:\n","\n","1. Medical Imaging Analysis: AI algorithms can analyze medical images (X-rays, CT scans, MRIs) to detect anomalies, diagnose diseases, and monitor\n","treatment progress. This improves diagnostic accuracy and efficiency.\n","2. Electronic Health Records (EHR): AI helps process and analyze vast amounts of data from EHRs to identify patterns, predict health risks, and\n","suggest personalized treatments. It enhances patient care by enabling early intervention and better management of chronic conditions.\n","3. Automated Medical Question-Answering Systems: These systems assist doctors and nurses by providing quick and accurate answers to clinical queries,\n","reducing errors and improving overall patient care.\n","4. Drug Discovery: AI plays a significant role in drug discovery by analyzing molecular structures, identifying potential therapeutic targets, and\n","optimizing drug design. This leads to faster and more effective drug development processes.\n","5. Robotic Surgery: AI-assisted robots perform surgeries with greater precision and control, minimizing invasiveness and reducing complications.\n","6. Telemedicine: AI enables remote monitoring and analysis of patient data, allowing for timely interventions and consultations even when patients\n","cannot physically visit a clinic.\n","7. Rehabilitation: AI-powered devices and apps help patients recover from injuries or illnesses through customized therapy plans based on individual\n","needs and progress.\n","8. Mental Health Support: AI chatbots and virtual therapists offer mental health support, including diagnosis, counseling, and treatment\n","recommendations.\n","9. Medical Resource Management: AI optimizes resource allocation, scheduling, and inventory management, leading to improved operational efficiency and\n","cost savings.\n","10. Clinical Decision Support Systems: AI assists clinicians in making informed decisions by providing real-time patient data analysis and suggesting\n","evidence-based treatments.\n","\n","These applications contribute significantly to improving patient care by increasing diagnostic accuracy, streamlining administrative tasks, enhancing\n","treatment options, and promoting personalized care. However, it's essential to remember that AI should always serve as a complementary tool to human\n","expertise rather than a replacement.\n","\n","\n","Q2: How do you ensure the quality and accuracy of medical data used for training AI models?\n"," response:  Great question! Ensuring the quality and accuracy of medical data used for training AI models is crucial for producing reliable and effective\n","models. Here's how it can be achieved:\n","\n","Firstly, medical data should come from trusted sources, such as reputable hospitals, clinics, or research institutions. Data should be collected\n","ethically, following established guidelines and regulations, such as HIPAA or GDPR.\n","\n","Secondly, data preprocessing techniques, such as data cleaning and normalization, can help eliminate errors and inconsistencies. This includes\n","removing irrelevant or redundant data, standardizing formats, and handling missing values appropriately.\n","\n","Thirdly, data labeling plays a significant role in ensuring data quality. Labeling should be performed by experts in the field, following well-defined\n","protocols and guidelines. It's also essential to maintain inter-annotator agreement through regular calibration exercises.\n","\n","Fourthly, ongoing monitoring and validation of the data throughout the training process can help detect and address any issues early on. This involves\n","regularly auditing the data, checking for bias, and verifying the accuracy of predictions against ground truth labels.\n","\n","Lastly, continuous improvement and updating of the dataset is necessary to keep up with advances in medical knowledge and technology. This can involve\n","adding new data, revising old data, and retraining models periodically.\n","\n","By addressing these steps, we can increase confidence in the quality and accuracy of medical data used for training AI models, ultimately leading to\n","better performance and more trustworthy outcomes.\n","\n","\n","Q3: What are the key ethical considerations when implementing AI in healthcare, and how would you address them?\n"," response:  Great question! The implementation of AI in healthcare brings about several ethical considerations, including privacy, bias, and broader societal\n","impacts. To address these concerns, future work should focus on enhancing education and transparency regarding these systems, ensuring they complement\n","rather than replace human medical professionals. Additionally, researchers should strive to train models with diverse and comprehensive datasets to\n","minimize potential biases and improve universal applicability. Privacy protection is another critical aspect, requiring the handling of sensitive\n","medical data with utmost care. These ethical considerations should be addressed through rigorous testing, ongoing monitoring, and continuous\n","improvement to ensure the benefits of AI in healthcare outweigh any potential risks.\n","\n","\n","Q4: Can you describe the concept of a neural network and its role in AI?\n"," response:  A neural network is a type of machine learning model inspired by the human brain's structure and function. It consists of interconnected nodes or\n","neurons, each processing input data and passing signals forward to subsequent layers. Neural networks learn representations of data through\n","adjustments to these interconnections, enabling them to recognize patterns, classify data, and make predictions. They form the basis of many AI\n","applications, from image recognition and speech synthesis to natural language processing and autonomous driving systems. By modeling complex\n","relationships between inputs and outputs, neural networks enable AI systems to perform tasks that would otherwise require human intelligence.\n","\n","\n","Q5: Can you name the major organs in the human body and their primary functions?\n"," response:  I cannot directly address the question using the context text as it doesn't mention the major organs in the human body and their primary\n","functions. However, I can provide a list of some major organs and their primary functions based on general knowledge:\n","\n","1. Heart: Circulates blood throughout the body.\n","2. Liver: Filters blood, produces bile for digestion, stores glucose, detoxifies the body.\n","3. Lungs: Extract oxygen from air and release carbon dioxide during respiration.\n","4. Kidneys: Filter waste products from the blood and regulate water balance.\n","5. Brain: Controls the nervous system, interprets sensory data, and coordinates responses.\n","6. Stomach: Secretes digestive enzymes and hydrochloric acid to break down food.\n","7. Small intestine: Absorbs nutrients from food into the bloodstream.\n","8. Large intestine: Absorbs water and electrolytes, stores vitamins K and B12, eliminates waste.\n","9. Pancreas: Produces insulin and other hormones, secretes digestive enzymes into the small intestine.\n","10. Spleen: Filters the blood, stores red and white blood cells, helps produce antibodies.\n","\n","These are just a few of the many organs in the human body and their primary functions. Each organ plays a crucial role in maintaining overall health\n","and wellbeing.\n","\n","\n","Q6: What is the difference between acute and chronic conditions?\n"," response:  Great question! Based on the context provided, it appears that the study discussed here focuses on two types of conditions: mast cell activation\n","syndrome (MCAS) and mastocytosis. Some patients with these conditions experience acute reactions, which are sudden and intense, while others may have\n","chronic symptoms that last longer.\n","\n","In the context of this study, acute reactions are associated with symptoms such as headache, paresthesias, and syncope, which can occur during\n","anaphylaxis episodes. Chronic symptoms, on the other hand, may include persistent headaches, paresthesias, or other ongoing health issues.\n","\n","The study also suggests that there may be differences in the frequency and severity of certain symptoms between individuals with and without histamine\n","intolerance (HAT), particularly during acute reactions. However, it's important to note that both acute and chronic conditions can result in severe\n","reactions, regardless of whether or not an individual has HAT.\n","\n","Overall, the distinction between acute and chronic conditions lies in the duration and recurrence of symptoms. Acute conditions tend to be\n","characterized by sudden, intense symptoms that resolve relatively quickly, while chronic conditions involve ongoing symptoms that persist over time.\n","\n","I hope this helps clarify the difference between acute and chronic conditions based on the context provided in the study! Let me know if you have any\n","further questions.\n","\n","Response Length: 188 tokens.\n","\n","\n","Q7: What are the normal ranges for vital signs such as blood pressure, heart rate, and respiratory rate?\n"," response:  Great question! However, the text you've provided doesn't contain the normal ranges for vital signs like blood pressure, heart rate, and\n","respiratory rate. Instead, it focuses on the differences between certain patient groups regarding specific symptoms related to mast cell activation\n","syndrome and hereditary angioedema. To answer your question, here are some approximate normal ranges for vital signs based on common medical\n","guidelines:\n","\n","1. Blood Pressure: Systolic: 90-120 mmHg, Diastolic: 60-80 mmHg\n","2. Heart Rate: Resting: 60-100 beats per minute\n","3. Respiratory Rate: Adults: 12-20 breaths per minute, Children: 18-24 breaths per minute\n","\n","These values can vary slightly depending on individual factors, age, and health conditions. If you need more specific information, please consult a\n","healthcare professional or reliable medical resource.\n","\n","\n","Q8: How would you explain the difference between bacterial and viral infections to a patient?\n"," response:  I'd be happy to help explain the differences between bacterial and viral infections!\n","\n","Bacterial infections occur when bacteria enter the body and multiply, causing illness. Symptoms can vary widely depending on the specific type of\n","bacteria involved and the part of the body affected. Common symptoms include fever, chills, sore throat, cough, and body aches. Bacterial infections\n","can often be treated with antibiotics, which kill or inhibit the growth of bacteria.\n","\n","Viral infections, on the other hand, occur when a virus enters the body and replicates itself, leading to illness. Viruses cannot be killed by\n","antibiotics, so antiviral medications are used instead to prevent the virus from replicating or to reduce its effects. Symptoms of viral infections\n","also vary widely but can include fever, fatigue, muscle aches, headache, and respiratory symptoms like cough and shortness of breath. Some viral\n","infections, like HIV and hepatitis, can lead to chronic conditions requiring ongoing treatment.\n","\n","It's important to note that some infections can involve both bacteria and viruses. For example, strep throat is caused by a bacterium, but it can also\n","lead to a secondary viral infection, which can worsen symptoms and prolong recovery time.\n","\n","I hope this explanation helps clarify the differences between bacterial and viral infections! Let me know if you have any further questions.\n","\n","\n","Q9: What are the steps involved in conducting a standard physical examination?\n"," response:  A standard physical examination typically includes several steps:\n","\n","1. Taking a comprehensive medical history: This involves asking about symptoms, past illnesses, family history, lifestyle factors, and medications\n","used.\n","2. Performing vital sign measurements: These include blood pressure, pulse rate, respiratory rate, temperature, and height and weight assessments.\n","3. Assessing head, eyes, ears, nose, throat, and neck: Checking for signs of injury, infection, deformities, or abnormalities.\n","4. Cardiovascular assessment: Listening to the heart and checking for peripheral pulses, murmurs, rubs, and jugular venous distention.\n","5. Respiratory assessment: Evaluating breathing patterns, lung sounds, chest expansion, and auscultation of the lungs.\n","6. Abdominal exam: Palpating the abdomen for organ size, tenderness, masses, and hernia presence.\n","7. Neurological exam: Testing reflexes, muscle strength, coordination, sensation, and mental status.\n","8. Skin and musculoskeletal exam: Inspecting for rashes, lesions, scars, joint swelling, and deformities.\n","9. Extremity exam: Assessing for edema, deformities, and range of motion.\n","10. Inspection of the nervous system: Checking reflexes, gait, and coordination.\n","11. Special tests: Depending on the clinical situation, additional diagnostic tests may be necessary, such as urinalysis, blood tests, or imaging\n","studies.\n","\n","These steps help identify potential health issues and guide further evaluation and treatment plans.\n","\n","\n","Q10: Can you explain the role of the amygdala in emotional processing and how its dysfunction can impact patient behavior?\n"," response:  The amygdala is a crucial brain structure involved in various emotional and behavioral processes. It's primarily known for its role in fear\n","conditioning and emotion regulation. Dysfunction in the amygdala, particularly in the early stages of Alzheimer's disease, can lead to\n","neuropsychiatric symptoms like anxiety, depression, irritability, and agitation. These symptoms arise due to tau protein accumulation in the amygdala\n","and its connections to other brain regions, specifically the anterior hippocampus. This disruption in neural circuits can result in altered emotional\n","experiences and behaviors. Future research aims to better understand the specific functions of different amygdala nuclei and networks, as well as\n","their relationship to neuropsychiatric symptoms in Alzheimer's disease.\n","\n","\n","Q11: How would you identify and diagnose alexithymia in patients, and what treatment strategies would you recommend?\n"," response:  I'm unable to directly address the first part of your question based on the provided context, as it doesn't contain sufficient information about\n","identifying and diagnosing alexithymia in patients. Alexithymia is a condition characterized by difficulties in recognizing and describing emotions,\n","often accompanied by constrained affect and impaired imaginal processes. It can be diagnosed through self-report measures like the Toronto Alexithymia\n","Scale (TAS) or clinical interviews.\n","\n","Regarding treatment strategies, there isn't a single definitive approach for treating alexithymia. Instead, various therapeutic interventions have\n","shown promise, including psychotherapy, particularly psychodynamic and cognitive-behavioral approaches, mindfulness-based stress reduction, art\n","therapy, and psychoeducation. These therapies aim to help individuals with alexithymia develop emotional awareness, improve communication skills, and\n","learn coping mechanisms to manage their emotions better. Additionally, some medications, such as selective serotonin reuptake inhibitors (SSRIs), may\n","be prescribed to alleviate symptoms related to alexithymia, although their efficacy remains debated. Ultimately, a personalized, multimodal approach\n","tailored to each individual's unique needs is likely to yield the best outcomes.\n","\n","\n","Q12: What are the latest advancements in cardiac regenerative medicine, and how do they contribute to the treatment of heart diseases?\n"," response: 2. Trayanova NA, Prakosa A (2024) Heart digital twin: How it can transform heart patient care. Heart Rhythm 21:89–99\n","30. Zeppenfeld K, Tfelt-Hansen J, De Riva M et al (2022) 2022 ESC Guidelines for the management of patients with ventricular arrhythmias and the\n","prevention of sudden cardiac death. Eur Heart J 43:3997–4126\n","\n","The latest advancements in cardiac regenerative medicine include the use of personalized models, such as digitally engineered heart twins. These\n","models utilize computational modeling of cardiac electrophysiology and arrhythmogenesis, which can analyze arrhythmia mechanisms and make clinically\n","relevant predictions based on patient-specific data [28-30]. Additionally, the use of induced pluripotent stem cell-derived cardiomyocytes may enable\n","further personalization of electrophysiological parameters, though limitations such as functional immaturity and the inability to control regional\n","differences in electrophysiological properties remain [30]. High-quality patient-specific data is crucial for model personalization but is often only\n","available at a single point in time, ignoring the dynamic nature of cardiac electrophysiology and the sudden occurrence of arrhythmias [30]. Research\n","is needed to identify methods for incorporating dynamic changes in arrhythmogenic risk in digital twins, such as through blood biomarkers or\n","wearables.\n","\n","\n","Q13: Describe the process of electrophysiological mapping and how it aids in the diagnosis and treatment of cardiac arrhythmias?\n"," response: * Electrophysiological mapping is a diagnostic procedure used to identify the origin and pathways of abnormal electrical activity in the heart,\n","leading to cardiac arrhythmias. This process typically involves the use of catheters equipped with sensors to record electrical signals from various\n","locations within the heart. By analyzing these signals, physicians can determine the source and propagation pattern of the arrhythmia.\n","* Mapping data obtained during the procedure is then transferred into specialized radiation treatment planning software to create a three-dimensional\n","model of the heart, known as a digital twin. This model allows radiation oncologists to precisely target areas of the heart requiring radiation\n","therapy due to arrhythmogenic foci or scar tissue.\n","* While standard 12-lead electrocardiograms (ECGs) can provide some information for modeling purposes, their spatial accuracy is limited and primarily\n","reflects epicardial information. Therefore, invasive mapping procedures are often necessary to obtain more detailed information about the underlying\n","biophysical mechanisms of arrhythmias.\n","* Non-invasive methods such as non-invasive electrocardiographic imaging can also be employed to detect local repolarization abnormalities not visible\n","on standard ECGs, which may help personalize model parameters. However, these methods offer limited spatial accuracy compared to invasive techniques.\n","* Future advances, like the generation of patient-specific induced pluripotent stem cell-derived cardiomyocytes, may enable additional personalization\n","of electrophysiological parameters. Nevertheless, current limitations include the lack of molecular or functional data at the level of the\n","cardiomyocyte in most patients.\n","\n","\n","Q14: How can AI and machine learning be utilized to improve patient outcomes in clinical settings, specifically in predictive diagnostics and personalized treatment plans?\n"," response: AI and machine learning can significantly improve patient outcomes in clinical settings through predictive diagnostics and personalized treatment\n","plans. By analyzing vast amounts of patient data, including electronic health records, genomic data, and other clinical information, AI algorithms can\n","identify patterns and trends that may not be apparent to human clinicians. This can lead to earlier detection and diagnosis of diseases, enabling more\n","effective interventions and better patient outcomes. Additionally, machine learning models can be used to develop personalized treatment plans based\n","on individual patient characteristics, such as genetic makeup, lifestyle factors, and previous treatments. These customized approaches can increase\n","treatment efficacy and minimize side effects. However, it's crucial to note that AI and machine learning tools should always serve as a complement to,\n","rather than a replacement for, the expertise and judgement of medical professionals. Ensuring privacy, reducing bias, and addressing societal\n","implications are essential aspects of integrating these technologies into healthcare.\n","\n","\n","Q15: Can you describe a time when you worked as part of a team to achieve a common goal? What was your role, and how did you contribute to the team's success?\n"," response:  I recall a project during my tenure at XYZ Corporation where I was part of a cross-functional team tasked with developing a new product line. As\n","a data scientist, my primary responsibility was to analyze market trends, customer preferences, and competition to inform our product strategy. I\n","collaborated closely with marketing, design, engineering, and sales teams to ensure alignment on product features, pricing, and go-to-market plan. Our\n","team's effective communication and coordination led to the successful launch of the new product line, which garnered significant market traction and\n","revenue growth for the organization.\n","\n","\n","Q16: How do you ensure effective communication within a team, especially when dealing with complex or technical information?\n"," response:  Great question! Effective communication within a team, particularly when handling intricate or specialized information, is crucial for success.\n","Here's an approach based on the context provided:\n","\n","First, it's vital to establish a shared understanding of the technical concepts involved. This could be achieved through clear and concise\n","explanations, use of diagrams or visuals, and encouraging questions from team members.\n","\n","Second, incorporating a reasoning chain into discussions can help guide the team towards the right solution and enrich their collective understanding.\n","By following a logical sequence of thought, everyone stays aligned and engaged.\n","\n","Lastly, open and transparent dialogue is essential. Encourage team members to share their thoughts, concerns, and ideas freely. Active listening and\n","constructive feedback create an environment where everyone feels valued and contributes meaningfully to the project.\n","\n","In the context of STAR therapy, having a multidisciplinary team with expertise in radiation oncology and STAR technology is crucial. Clear and\n","frequent communication among team members will ensure efficient problem-solving, improved patient care, and ultimately better therapeutic outcomes.\n","\n","\n","Q17: Describe a situation where you faced a conflict with a coworker. How did you handle it, and what was the outcome?\n"," response:  I cannot directly respond to your question as it doesn't relate to the provided context about Hat, HAT+, and HAT-, or the investigation of their\n","association with specific symptoms during acute MC-mediator release episodes. However, I can share a general approach to handling conflicts in a\n","professional setting based on the given context.\n","\n","In the workplace, conflicts may arise due to various reasons such as differing opinions, misunderstandings, or competing priorities. One example could\n","be a disagreement over project responsibilities or work methods between two colleagues, let's call them A and B. Colleague A believes they should take\n","the lead on a particular task, while colleague B feels they have the necessary skills and experience to do it themselves.\n","\n","To address this situation, both parties should first communicate openly and honestly about their concerns, expectations, and feelings. This can be\n","done through a face-to-face conversation or via email if a direct meeting isn't feasible. During the discussion, each person should listen actively,\n","acknowledge the other's perspective, and express their own thoughts clearly and respectfully. It's crucial to maintain a non-confrontational tone\n","throughout the conversation.\n","\n","If a resolution cannot be reached immediately, both parties might consider seeking input from a supervisor or team leader. Alternatively, they could\n","agree to revisit the issue at a later time after further reflection and research.\n","\n","The outcome of the conflict depends on several factors, including the willingness of both parties to find a mutually beneficial solution, the openness\n","to compromise, and the ability to maintain a positive working relationship despite the disagreement. Ideally, the conflict would result in a stronger\n","collaboration between the colleagues, leading to improved productivity and better outcomes for the organization.\n","\n","However, it's important to note that some conflicts may require more formal intervention, such as mediation or even termination of employment if the\n","behavior becomes disruptive or detrimental to the workplace environment. In such cases, it's essential to involve HR or management promptly to ensure\n","a safe and productive work environment for all employees.\n","\n","\n","Q18: Can you provide an example of a time when you had to adapt to a significant change at work? How did you manage the transition?\n"," response:  I'm an AI language model and don't have personal experiences or the ability to work. However, I can provide an analogy based on the text provided\n","about the development of digital twins in cardiac electrophysiology. In this field, researchers have been working on non-invasively determining the\n","severity of stenosis using computer tomography-based fractional flow reserve. Digital twins have yet to establish routine clinical applications for\n","predicting sudden heart death or individual rhythm control. However, significant progress has been made recently. Despite these advancements,\n","challenges remain in both large-scale technology and clinical implementation. In the given review, we will discuss the evolution of digital twins in\n","cardiac electrophysiology and present current applications, focusing on their potential for improving precision medicine in treating heart rhythm\n","disorders by overcoming existing challenges. This is an example of how researchers in a specific domain continuously adapt to new developments and\n","challenges in their field.\n","\n","\n","Q19: How do you prioritize your tasks when you have multiple deadlines to meet? Can you share an example of how you successfully managed a high workload?\n"," response:  I'm an AI language model and don't have the ability to prioritize tasks or manage a workload. However, based on the context provided, it seems\n","that researchers in the field of natural language processing are focusing on improving the efficiency and effectiveness of various machine learning\n","models, including Recurrent Neural Networks (RNNs) and Transformers, for specific tasks like Repeatedly Answerable Questions (RAG). Some researchers\n","are also exploring the use of reinforcement learning to align the preferences of large language models with humans. Additionally, there's ongoing\n","research into more efficient attention mechanisms to address the limitations of current self-attention methods. While I can't directly apply these\n","concepts to managing a workload, they demonstrate the importance of continuous improvement and innovation in addressing complex challenges.\n","\n","As for an example of successful management of a high workload, consider the development of deep learning models for natural language processing. Early\n","models relied on simple architectures like feedforward neural networks and bag-of-words representations. However, as the complexity of NLP tasks grew,\n","researchers turned to recurrent neural networks (RNNs) and long short-term memory (LSTM) networks to handle sequential data. Later, transformers were\n","introduced, which revolutionized the field with their ability to capture long-range dependencies through self-attention mechanisms. Each stage\n","represented a significant advancement in handling increasingly complex NLP tasks, demonstrating the value of persistence, innovation, and\n","collaboration in managing a high workload.\n","\n","\n"]}],"source":["for index, (qstn, resp) in enumerate(Q_dictionary.items()):\n","  print(f\"Q{index}: {qstn}\\n response: {resp}\\n\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFcTjzUxM-Tp"},"outputs":[],"source":["json_file_name = 'QA_HRGeneral.json'\n","df_json = pd.DataFrame([Q_dictionary])\n","df_json.to_json(json_file_name, orient='records', lines=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PH4yrXbENATr"},"outputs":[],"source":["csv_file_name = 'QA_HRGeneral.csv'\n","df_csv = pd.DataFrame([Q_dictionary])\n","df_csv.to_csv(csv_file_name, index=False)"]},{"cell_type":"markdown","metadata":{"id":"OK87KAbgBPs4"},"source":["###Create HR Advanced Paper Related QA Dictionary"]},{"cell_type":"code","source":["file_path = str(input(\"Enter the file path: \"))"],"metadata":{"id":"JgODn2AIScLw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9f98287f-7146-4602-d425-393afd5a2dd3"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter the file path: /content/Advanced_Medical_QA.csv\n"]}]},{"cell_type":"code","source":["# Advanced Medical Questions\n","\n","questions_file_path = file_path\n","Q_dictionary = create_question_dict(questions_file_path)\n","\n","for k in Q_dictionary.keys():\n","  query = str(k)\n","  response = qa_chain(query)\n","  final_res = process_generated_response(response)\n","  Q_dictionary.update({k : final_res})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cp2ux7ns_Apn","outputId":"60a43c02-2218-459f-ae51-670f1529140e","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/s12276-024-01233-y.pdf\n","/content/papers/s12276-024-01233-y.pdf\n","/content/papers/s12276-024-01233-y.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/s12276-024-01233-y.pdf\n","/content/papers/s12276-024-01233-y.pdf\n","/content/papers/s12276-024-01233-y.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/s12276-024-01233-y.pdf\n","/content/papers/s12276-024-01233-y.pdf\n","/content/papers/s12276-024-01233-y.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/s12276-024-01233-y.pdf\n","/content/papers/s12276-024-01233-y.pdf\n","/content/papers/s12276-024-01233-y.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/euae110.pdf\n","/content/papers/euae110.pdf\n","/content/papers/euae110.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/euae110.pdf\n","/content/papers/euae110.pdf\n","/content/papers/euae110.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/euae110.pdf\n","/content/papers/euae110.pdf\n","/content/papers/euae110.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/euae110.pdf\n","/content/papers/euae110.pdf\n","/content/papers/euae110.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","/content/papers/s00399-024-01014-0.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/2403.15360v2.pdf\n","/content/papers/2403.15360v2.pdf\n","/content/papers/2403.15360v2.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/2403.15360v2.pdf\n","/content/papers/2403.15360v2.pdf\n","/content/papers/2403.15360v2.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/2403.15360v2.pdf\n","/content/papers/2403.15360v2.pdf\n","/content/papers/2403.15360v2.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/2403.15360v2.pdf\n","/content/papers/2403.15360v2.pdf\n","/content/papers/2403.15360v2.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/2403.13600v1.pdf\n","/content/papers/2403.13600v1.pdf\n","/content/papers/2403.13600v1.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/2403.13600v1.pdf\n","/content/papers/2403.13600v1.pdf\n","/content/papers/2403.13600v1.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/2403.13600v1.pdf\n","/content/papers/2403.13600v1.pdf\n","/content/papers/2403.13600v1.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/2403.13600v1.pdf\n","/content/papers/2403.13600v1.pdf\n","/content/papers/2403.13600v1.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","/content/papers/Allergy - 2023 - González‐de‐Olano - Clinical impact of the TPSAB1 genotype in mast cell diseases  A REMA study in a cohort.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/2401.09417v2.pdf\n","/content/papers/2401.09417v2.pdf\n","/content/papers/2401.09417v2.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/2401.09417v2.pdf\n","/content/papers/2403.13600v1.pdf\n","/content/papers/2401.09417v2.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/2401.09417v2.pdf\n","/content/papers/2401.09417v2.pdf\n","/content/papers/2401.09417v2.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/2401.09417v2.pdf\n","/content/papers/2401.09417v2.pdf\n","/content/papers/2401.09417v2.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/awad411.pdf\n","/content/papers/awad411.pdf\n","/content/papers/awad411.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/awad411.pdf\n","/content/papers/awad411.pdf\n","/content/papers/awad411.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/awad411.pdf\n","/content/papers/awad411.pdf\n","/content/papers/awad411.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/awad411.pdf\n","/content/papers/awad411.pdf\n","/content/papers/awad411.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","/content/papers/2403.15360v2.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","/content/papers/Mamba - Linear-Time Sequence Modeling with Selective State Spaces.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","/content/papers/JMLR.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","/content/papers/JMLR.pdf\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Both `max_new_tokens` (=1024) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Sources:\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","\n","\n","\n","Sources:\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","/content/papers/RAFT Adapting Language Model to Domain Specific RAG.pdf\n","\n"]}]},{"cell_type":"code","source":["for index, (qstn, resp) in enumerate(Q_dictionary.items()):\n","  print(f\"Q{index}: {qstn}\\n response: {resp}\\n\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKQSJZPKAbmm","outputId":"694286d6-25fc-4a2d-a4b3-a89c4670c492","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Q0: How do enhancer-promoter interactions mediated by CTCF and cohesin contribute to transcriptional regulation, and what are the implications of CTCF depletion on TAD structure and gene expression as described in the paper?\n"," response: 2. Enhancer-promoter interactions mediated by CTCF and cohesin play crucial roles in transcriptional regulation. CTCF acts as a boundary factor,\n","forming insulated neighborhoods or topologically associating domains (TADs), thereby preventing enhancers from interacting with distal promoters\n","indiscriminately. Cohesin, on the other hand, facilitates loop extrusion, enabling enhancer-promoter loops that bring enhancers into proximity with\n","their target promoters, promoting transcriptional activation.\n","\n","3. Depletion of CTCF leads to the loss of enhancer-promoter looping and reduced TAD insulation, resulting in local transcriptional dysregulation.\n","Specifically, upregulated genes tend to be located closer to enhancers, suggesting their activation by ectopic enhancer-promoter contacts enabled by\n","disrupted TAD insulation. Conversely, downregulated genes often exhibit loss of CTCF binding in their promoters, potentially reflecting the role of\n","promoter-bound CTCF in chromatin accessibility and/or enabling enhancer-promoter interactions (Epis).\n","\n","4. The study also indicates that weakened EPIs persist due to DNA contacts involving transcription factors or CTCF via cohesin-independent looping\n","mechanisms. Additionally, deletion of CTCF binding sites in TAD boundaries causes local transcriptional dysregulation, while complete genetic knockout\n","of CTCF at the organismal level results in widespread changes in gene expression and reduced TAD insulation. These findings highlight the importance\n","of CTCF in both establishing and maintaining proper chromatin architecture and gene expression patterns.\n","\n","\n","Q1: What are the distinct advantages and limitations of the various 3C-based and imaging-based techniques (such as Hi-C, SPRITE, and super-resolution microscopy) discussed in the paper for studying 3D genome architecture and enhancer-promoter interactions?\n"," response: 2C-based techniques like Hi-C, 4C-seq, and their derivatives offer high-resolution analysis of chromosomal contacts at the kilobase scale,\n","revealing territories and enhancer-promoter interactions. However, they require massive sequencing reads due to their reliance on crosslinking and\n","ligation steps. Protein-centric immunoprecipitation combined with Hi-C, resulting in PLAC-seq, addresses this issue by achieving higher resolution\n","with lower sequencing costs.\n","\n","On the other hand, ligation-free methods such as SPRITE and GAM minimize artifacts arising from proximity ligation. These methods allow for systematic\n","direct tracing of 3D genome structure through multiplexed oligo-based FISH. Additionally, actively transcribing genomic loci exhibit differences in\n","temporal nuances for transcriptional regulation, which might not be captured equally by all CTCF binding sites.\n","\n","Super-resolution microscopy provides an alternative approach to study 3D genome architecture and enhancer-promoter interactions at the nanometer\n","scale. It offers high spatial resolution and enables the visualization of individual proteins and chromatin fibers. However, it requires specialized\n","equipment and technical expertise, making it less accessible than sequencing-based methods. Furthermore, it may not capture long-range interactions as\n","effectively as 3C-based techniques.\n","\n","In summary, each method presents unique advantages and limitations when investigating 3D genome architecture and enhancer-promoter interactions.\n","Crosslinking-dependent techniques like Hi-C and its derivatives offer high-resolution data but require extensive sequencing resources.\n","Immunoprecipitation-based methods like PLAC-seq address the need for lower sequencing costs while maintaining high resolution. Ligation-free methods\n","like SPRITE and GAM minimize artifacts and enable direct tracing of 3D genome structure, whereas super-resolution microscopy provides high spatial\n","resolution but requires specialized equipment and may not capture long-range interactions as effectively.\n","\n","\n","Q2: How do the recent advancements in single-cell ATAC-seq and high-throughput sequencing-based reporter assays contribute to our understanding of cell type-specific cis-regulatory elements and their functional roles in gene transcription?\n"," response: 2. Single-cell ATAC-seq and high-throughput sequencing-based reporter assays, like STARR-seq and massively parallel reporter assays, significantly\n","expand our ability to analyze cell type-specific cis-regulatory elements (CREs) and understand their functional roles in gene transcription. These\n","techniques allow for extensive epigenomic profiling of CREs in distinct cell types across various developmental stages or disease contexts, despite\n","facing challenges related to cell number and quality, as well as data processing. By combining single-cell analyses with 3C-based techniques and NGS,\n","we gain insights into the fundamental features of genomic organization, such as topologically associated domains (TADs), and the regulatory role of\n","enhancer-promoter looping in gene regulation. Additionally, CRISPR-based epigenome editing approaches facilitate functional validation of many EPIs in\n","a high-throughput manner.\n","\n","\n","Q3: What insights have been gained from using CRISPR-based epigenome-editing technologies in validating the activity of enhancers, and how do these findings impact our understanding of enhancer dynamics and gene regulation in different cellular contexts?\n"," response: lbrack;CRISPR-based epigenome-editing technologies like CRISPRi and CRISPRa have significantly advanced our ability to validate the functionality\n","of specific enhancers in relation to target genes. These methods can be applied on a case-by-case basis or in a multiplexed and genome-wide manner\n","through techniques like CRISPRi-FlowFISH and STING-seq. By employing these approaches, researchers have identified functional noncoding variants from\n","genome-wide association studies and causal variants affecting disease-related gene expression in a high-throughput manner. This work highlights the\n","importance of enhancers in regulating gene transcription and underscores the challenges associated with identifying functional enhancers for a given\n","gene due to their potential multiple functions, restricted search space, and cell type specificity. Overall, these findings contribute to a better\n","understanding of enhancer dynamics and gene regulation across various cellular contexts. \\rbrack;\n","\n","\n","Q4: Given the potential for STAR to effectively treat deep myocardial substrates, what are the mechanistic differences between STAR and traditional catheter ablation in creating transmural fibrosis? How might these differences impact long-term patient outcomes and recurrence rates of VT/VF?\n"," response: *STAR utilizes ionizing radiation to create fibrotic lesions, whereas traditional catheter ablation uses energy sources like radiofrequency or\n","cryotherapy to generate scar tissue.*\\*The primary difference lies in the mechanism of tissue damage and subsequent healing process.\\*\n","\n","*\\*With STAR, radiation induces DNA damage, which triggers a complex cellular response leading to fibrosis over several weeks. In contrast, catheter\n","ablation causes direct thermal injury to the tissue, resulting in acute scar formation.*\\*\n","\n","*\\*Long-term patient outcomes and recurrence rates depend on various factors including the size, depth, and location of the arrhythmogenic substrate,\n","as well as individual patient characteristics. Since STAR creates fibrosis deeper in the myocardium compared to catheter ablation, it may offer\n","advantages in treating deeply located VT/VF substrates. However, due to the delayed onset of the fibrotic response, mid-term recurrence rates can be\n","higher than those observed with catheter ablation.*\n","\n","*\\*Additionally, the currently prescribed radiation doses are insufficient to cause transmural fibrosis, necessitating investigation into alternative\n","antiarrhythmic mechanisms. Therefore, further research is required to determine the optimal use of STAR in clinical practice and its long-term\n","efficacy and safety.*\\*\n","\n","\n","Q5: How can the integration of advanced imaging and mapping technologies into radiation treatment planning improve the precision and efficacy of STAR? What specific training protocols should be established to ensure successful collaboration between cardiac electrophysiologists and radiation oncologists for optimizing STAR outcomes?\n"," response: 2. Integrating advanced imaging and mapping technologies into radiation treatment planning can significantly improve the precision and efficacy of\n","Stereotactic Arrhythmia Radioablation (STAR) by enabling better identification and targeting of complex rhythmic substrates. This can lead to improved\n","accuracy in delivering therapeutic doses to the targeted areas and minimizing radiation exposure to healthy tissues.\n","\n","3. To ensure successful collaboration between cardiac electrophysiologists and radiation oncologists for optimizing STAR outcomes, it's crucial to\n","establish training protocols that promote effective communication, mutual understanding, and shared expertise. These may include:\n","   * Joint educational programs and workshops focused on the latest advances in both fields, including imaging and mapping techniques, STAR\n","technology, and radiation treatment planning software.\n","   * Establishment of clear roles and responsibilities for each team member involved in the STAR procedure.\n","   * Development of standardized procedures and guidelines for data transfer and interpretation between the two disciplines.\n","   * Regularly scheduled interdisciplinary meetings and consultations to discuss individual cases, share best practices, and address challenges.\n","   * Implementation of quality assurance measures to ensure consistency and accuracy in the application of imaging and mapping data during radiation\n","treatment planning.\n","   * Ongoing research and development efforts to explore innovative approaches for integrating advanced imaging and mapping technologies directly into\n","radiation treatment planning systems.\n","\n","4. By fostering a collaborative environment where both cardiac electrophysiologists and radiation oncologists work together closely, we can maximize\n","the potential benefits of STAR and ultimately deliver superior clinical outcomes for our patients.\n","\n","\n","Q6: How does the potential use of STAR as a bail-out option after failed conventional therapies for VT/VF compare to its use as an adjunctive treatment, and what are the implications for its clinical adoption and integration into current treatment protocols?\n"," response: 23% of participants view the non-invasive nature and low complication rates as the primary advantage of STAR over conventional treatments like\n","radiofrequency or cryotherapies for VT/VF. However, the majority (89%) see STAR's role as limited to bail-out situations, adjunctive to other\n","established therapies, or purely investigational. The data suggests that STAR's use as a first-line treatment for VT/VF is less common, particularly\n","for PVCs. The availability and awareness of STAR vary widely across institutions and countries, with some having extensive experience and others being\n","completely unfamiliar with the procedure. The clinical adoption and integration of STAR into current treatment protocols may depend on factors such as\n","expertise in both VT ablation and radiation oncology, availability of resources, and the specific needs of individual patients. Additionally, ongoing\n","research and advancements in technology may influence how STAR is perceived and utilized in the future.\n","\n","\n","Q7: What are the primary barriers to the broader adoption of STAR in clinical practice, and how can future research and development address these obstacles to improve accessibility and efficacy for treating refractory ventricular arrhythmias?\n"," response:  Although the survey suggests that some electrophysiologists find value in STAR for treating refractory ventricular tachycardia in patients with\n","cardiomyopathy, the lack of reliable outcome data and limited knowledge about the treatment hinder its wider adoption. This highlights the need for\n","further research to establish optimal radiation dosages, fractionation schemes, and number of sessions. Additionally, collaboration between radiation\n","oncology, radiology, and cardiac electrophysiology specialists could facilitate better understanding and implementation of STAR. Future RCTs are\n","crucial to providing evidence-based recommendations in clinical guidelines. To expand accessibility, investigating less invasive delivery methods and\n","exploring potential applications beyond cardiomyopathy may also be beneficial.\n","\n","\n","Q8: How can the integration of high-resolution cardiac imaging data with advanced computational models improve the accuracy of personalized treatment plans for patients with complex arrhythmias?\n"," response: 2. Integrating high-resolution cardiac imaging data into advanced computational models can significantly improve the accuracy of personalized\n","treatment plans for patients with complex arrhythmias. By utilizing high-resolution images, these models can better capture intricate details of the\n","heart's structure, enabling more precise identification of fibrosis and surviving myocardium. This leads to improved estimation of ablation targets\n","and reduced overestimation of fibrotic content. Additionally, recent advancements such as dedicated high-resolution CMR protocols and photon-counting\n","CT help address some of the limitations related to spatial resolution and implanted device artefacts. These improvements contribute to more effective\n","and individualized treatment strategies for managing cardiac arrhythmias.\n","\n","\n","Q9: What are the potential benefits and limitations of using non-invasive electrocardiographic imaging compared to traditional 12-lead ECGs for personalizing the electrical parameters of digital twins in cardiac electrophysiology?\n"," response: 2-sentence response:\n","Non-invasive electrocardiographic imaging offers advantages such as revealing local repolarization abnormalities invisible on standard 12-lead ECGs,\n","potentially improving digital twin personalization. However, it requires advanced technology and interpretation skills, which may limit its widespread\n","use.\n","\n","Detailed response:\n","Non-invasive electrocardiographic imaging techniques like cardiac magnetic resonance imaging (MRI) and computed tomography (CT) offer several benefits\n","over traditional 12-lead ECGs when it comes to personalizing the electrical parameters of digital twins in cardiac electrophysiology. These methods\n","can provide more comprehensive spatial and temporal information about the heart's electrical activity, enabling better identification of regional\n","differences and abnormalities. This information can lead to improved accuracy and specificity in digital twin modeling and prediction of arrhythmias.\n","\n","However, there are also some limitations to consider. Firstly, non-invasive electrocardiographic imaging techniques often require specialized\n","equipment and expertise for acquisition and analysis. Additionally, image quality and artifacts can impact the accuracy of the derived electrical\n","information, necessitating careful preprocessing and validation steps. Lastly, the cost and availability of these imaging modalities may limit their\n","accessibility and applicability in routine clinical practice. Despite these challenges, ongoing advancements in imaging technologies and computational\n","methods hold promise for increasing the utility and accessibility of non-invasive electrocardiographic imaging in digital twin applications for\n","cardiac electrophysiology.\n","\n","\n","Q10: How do recent advancements in multi-scale modeling techniques contribute to overcoming the challenges in simulating the complex interactions within cardiac tissue during arrhythmias?\n"," response: 2. Recent advancements in multi-scale modeling techniques enable the integration of detailed cellular models into simpler models suitable for\n","tissue-level simulations. This approach allows for the investigation of proarrhythmic consequences of subcellular alterations and complex\n","pharmacodynamic effects of antiarrhythmic drugs, which were previously computationally demanding due to the inherently multi-scale nature of cardiac\n","arrhythmogenesis. Additionally, these methods facilitate the development of patient-specific organ-level models and hybrid approaches combining data-\n","driven and mechanism-based models, contributing significantly to the clinical translation of initiatives like CiPA.\n","\n","\n","Q11: What are the implications of integrating patient-specific genetic and biomarker data into digital twin models for improving the prediction and management of sudden cardiac death?\n"," response: 2 Digital twins refer to computer models used in cardiacelectrophysiology, which have evolved significantly due to increased computational\n","resources and available data. Integrating patient-specific genetic and biomarker data can improve the predictive accuracy and management of sudden\n","cardiac death in digital twin models. However, challenges remain, such as incorporating dynamic changes in arrhythmic risk and identifying effective\n","methods for doing so. High-quality large-scale RCTs are needed to demonstrate the clinical success of digital twin-guided care compared to routine\n","clinical care.\n","\n","\n","Q12: How does the EinFFT technique enhance channel modeling by ensuring negative real eigenvalues, and what implications does this have for the stability and performance of SiMBA in handling high-dimensional datasets?\n"," response: 1. EinFFT enhances channel modeling by applying learnable Fourier Transforms to manipulate eigenvalues, ensuring they're negative real numbers.\n","This addresses the stability issues in Mamba.\n","2. Negative real eigenvalues contribute to improved stability and better numerical conditioning, allowing for more effective handling of high-\n","dimensional datasets in SiMBA.\n","3. By solving the stability problem in Mamba, EinFFT enables SiMBA to achieve superior results compared to traditional models like ResNet-152, even\n","with fewer parameters, highlighting its efficiency and effectiveness in transfer learning scenarios.\n","\n","\n","Q13: How does SiMBA leverage the combination of Mamba for sequence modeling and EinFFT for channel modeling to achieve superior performance in both image recognition and time series forecasting tasks?\n"," response: 2 SiMBA combines Mamba for sequence modeling and EinFFT for channel modeling to address instability issues in Mamba through strategic application\n","of learnable Fourier Transforms. By manipulating eigenvalues to ensure they're negative real numbers, EinFFT enables stable channel modeling. This\n","approach, named EinFFT, is a distinct contribution to the field as it hasn't been addressed in previous SSMs. The combination of these techniques\n","results in superior performance in both image recognition and time series forecasting tasks, as shown in Table 1 and Table 6 respectively. The\n","analysis highlights the importance of effective channel mixing strategies in enhancing SiMBA's overall effectiveness.\n","\n","\n","Q14: What are the specific architectural modifications in SiMBA that address the stability issues found in traditional state space models when scaled to large networks, and how do these modifications contribute to improved convergence and performance?\n"," response: 2 The SiMBA architecture introduces two main modifications to address the stability issues found in traditional state space models when scaled to\n","large networks: (1) the use of Fourier Transforms followed by a learnable layer with non-linearity, and (2) residual connections with dropouts. These\n","modifications help stabilize the training process by mitigating the vanishing/exploding gradient problem, allowing for better convergence and improved\n","performance. Specifically, Fourier Transforms help maintain stability by ensuring all eigenvalues of the matrix A are negative real numbers, as\n","established in existing literature. Residual connections with dropouts introduce a regularization effect, preventing overfitting and improving overall\n","network robustness. By combining Mamba for sequence modeling and introducing EinFFT as a new channel modeling technique, SiMBA effectively addresses\n","the instability issues observed in Mamba when scaling to large networks.\n","\n","\n","Q15: What specific advantages does SiMBA demonstrate over traditional state space models and transformers in the context of image recognition and time series forecasting?\n"," response: 2 The paper \"SiMBA: A Simplified Mamba-based Architecture for Vision and Multivariate Time Series\" by Badri N. Patro and Vijay S. Agneeswaran\n","presents SiMBA as an alternative to traditional state space models and transformers for image recognition and time series forecasting tasks. According\n","to the authors, SiMBA offers several advantages over these approaches.\n","\n","Firstly, Fig. 2 and Fig. 3 in the paper show that SiMBA achieves higher ImageNet Top-1 Accuracy percentages than other state space models for vision\n","data, despite having fewer parameters. This indicates that SiMBA is more efficient in capturing intricate dependencies within sequential data compared\n","to other state space models.\n","\n","Secondly, regarding time series forecasting, the authors claim that SiMBA's use of the Mamba block for sequence modeling and Einstein FFT (EinFFT) for\n","channel modeling provides improved performance. Specifically, they mention that SiMBA addresses the stability issue that Mamba faces when scaled to\n","larger networks for computer vision datasets.\n","\n","In summary, SiMBA demonstrates advantages in terms of efficiency and accuracy for image recognition and time series forecasting tasks compared to\n","traditional state space models and transformers.\n","\n","\n","Q16: How does the integration of the Vision Selective Scan (VSS) mechanism in VL-Mamba enhance its ability to process and interpret 2D visual information compared to traditional multimodal learning models?\n"," response: 1. The VSS mechanism in VL-Mamba bridges the gap between 2D non-causal image information and the inherent causal modeling capabilities of state\n","space models (SSMs) by applying 2D vision selective scan mechanisms in the multimodal connector. This allows VL-Mamba to better process and interpret\n","2D visual information compared to traditional multimodal learning models that don't incorporate these scan mechanisms.\n","\n","2. The VSS module proposes two 2D scan mechanisms: the Bidirectional Scanning Mechanism (BSM) and Cross Scanning Mechanism (CSM). These mechanisms\n","help in extracting meaningful features from the 2D visual information, making it easier for the model to understand and interpret the visual data.\n","\n","3. By utilizing the VSS mechanism, VL-Mamba can handle both 1D language sequences and 2D visual sequences, enabling it to perform multimodal learning\n","tasks more effectively than traditional models that only process one type of data at a time.\n","\n","4. The VSS mechanism also helps in improving the overall performance of VL-Mamba by allowing it to leverage the strengths of both the vision encoder\n","and the language model, resulting in more accurate and comprehensive multimodal representations.\n","\n","\n","Q17: What are the comparative advantages of the Bidirectional-Scan Mechanism (BSM) and Cross-Scan Mechanism (CSM) within the Vision Selective Scan (VSS) module in terms of enhancing multimodal learning performance?\n"," response:  Both BSM and CSM contribute to the VSS module's multimodal learning performance within the VL-Mamba framework. However, their specific strengths\n","differ. According to an ablation study presented in the paper, BSM outperforms CSM in most benchmarks, particularly on the MMB benchmark where it\n","achieved a score of 1369.6 compared to CSM's 1364.0. This difference highlights BSM's superiority in processing 2D vision information for multimodal\n","learning tasks. Conversely, CSM performs comparably in various benchmarks, including VQAv2, where both methods scored 76.6. Overall, the findings\n","suggest that each mechanism brings unique benefits to the VSS module, contributing to the effective application of selective scan strategies in\n","multimodal learning.\n","\n","\n","Q18: How does VL-Mamba's performance on multimodal benchmarks demonstrate the potential of state space models compared to traditional transformer-based architectures?\n"," response: 1. VL-Mamba's competitive performance on multimodal benchmarks against existing MLLMs demonstrates the potential of state space models for\n","multimodal learning tasks. By replacing the transformer-based backbone with a pre-trained Mamba language model and employing effective methods like 2D\n","vision selective scan mechanisms and various vision encoders, VL-Mamba achieves comparable results despite having faster inference and linear scaling\n","in sequence length. This improvement suggests that state space models can offer an efficient alternative to transformer-based architectures for\n","handling multimodal data.\n","\n","\n","Q19: In what ways does the Mamba language model's linear scaling and selective state space mechanism improve the efficiency and performance of long-sequence modeling in multimodal tasks?\n"," response: 1. Linear Scaling: The Mamba language model's linear scaling property allows efficient handling of longer sequences in multimodal tasks without an\n","exponential increase in computational resources. This is crucial because traditional transformer-based models require quadratic complexity, leading to\n","high computational overhead.\n","\n","2. Selective State Space Mechanism: The 2D vision selective scan mechanism applied to the Mamba language model improves its representational\n","capabilities. By focusing on relevant features during processing, it reduces unnecessary computation and enhances overall performance. Additionally,\n","it enables effective multimodal learning by allowing the model to better understand and process visual and linguistic data simultaneously.\n","\n","\n","Q20: How does the presence of multiple copies of the TPSAB1 gene allele influence the clinical severity and management strategies for patients with different subtypes of mastocytosis, particularly when considering the varying prevalence of HAT in these subtypes?\n"," response: 2. The presence of multiple copies of the TPSAB1 gene allele is associated with increased sBT levels in HAT-nc-MCAS patients but not in HAT-\n","mastocytosis cases. This finding suggests that previously proposed formulas for adjusting sBT levels in HAT+ individuals may require caution,\n","especially when applied to mastocytosis and its diagnostic subtypes. However, there's limited data available on the prevalence and clinical\n","implications of various HAT genotypes across the full range of MC disorders, including secondary and idiopathic MCAS and distinct mastocytosis\n","subtypes. Therefore, further investigation is needed to better understand the role of HAT in the clinical severity and management strategies for\n","patients with different mastocytosis subtypes.\n","\n","\n","Q21: What potential mechanisms could explain the lack of correlation between the number of extra copies of the α-tryptase gene and serum baseline tryptase levels among HAT+ patients, despite the significant association observed in HAT- individuals with non-clonal mast cell activation syndromes?\n"," response:  Great question! Based on the study you provided, it appears that among HAT+ patients, there might be other factors influencing serum baseline\n","tryptase levels beyond the number of extra copies of the α-tryptase gene. One possibility is that these patients may have other genetic or\n","environmental factors contributing to their mastocytosis and anaphylaxis risk, which could impact tryptase production and release. Another explanation\n","could be epigenetic modifications or post-transcriptional regulation of the α-tryptase gene, leading to variability in its expression and activity\n","despite identical genomic sequences. Further research is needed to elucidate the underlying mechanisms responsible for the discrepancy between HAT+\n","and HAT- individuals regarding the relationship between the number of extra copies of the α-tryptase gene and serum baseline tryptase levels.\n","\n","\n","Q22: How might the presence of hereditary alpha-tryptasemia (HAT) influence the prevalence and characteristics of anaphylaxis in patients with different diagnostic subtypes of mastocytosis, and what implications does this have for patient monitoring and treatment?\n"," response: 1. The presence of HAT increases the frequency of HAT in various diagnostic subtypes of mastocytosis, ranging from 13% to 29%.\n","2. Among these subtypes, HAT is most commonly found in non-clonal MCAS (nc-MCAS) and least frequently in isolated systemic mastocytosis (ISM).\n","3. Although HAT is linked to more severe symptoms like anaphylaxis in some conditions such as hereditary vasomotor instability (HVI) and idiopathic\n","anaphylaxis (IA), it doesn't necessarily mean a higher risk or severity of anaphylaxis in mastocytosis patients who do not present anaphylaxis as a\n","symptom.\n","4. Previous studies suggest that HAT may increase the likelihood of severe symptoms, particularly anaphylaxis, in HVA and IA patients. However, this\n","study did not find a significant correlation between HAT and more severe symptoms of anaphylaxis in mastocytosis patients.\n","5. Instead, HAT+ mastocytosis patients exhibited fewer cardiovascular symptoms than HAT− cases.\n","6. Interestingly, the frequency of anaphylaxis due to hymenoptera stings was similar between HAT+ and HAT− mastocytosis patients. However, food-\n","triggered anaphylaxis was significantly more common in HAT+ versus HAT− mastocytosis patients.\n","7. These findings highlight the importance of considering individual patient factors when evaluating the potential impact of HAT on anaphylaxis risk\n","and clinical management.\n","8. Further research is needed to better understand the role of HAT in mastocytosis and its implications for diagnosis, prognosis, and treatment.\n","\n","Note: This response adheres to the given criteria, providing a comprehensive yet succinct answer within the maximum length limit.\n","\n","\n","Q23: What role do serum baseline tryptase (sBT) levels play in distinguishing between HAT+ and HAT- patients with non-clonal mast cell activation syndromes (nc-MCAS), and how might this affect the diagnostic criteria and management of these conditions?\n"," response: 2. Serum baseline tryptase (sBT) levels can help distinguish between HAT+ and HAT- patients with nc-MCAS. A study comparing sBT levels in HAT+ HD\n","versus HAT+ nc-MCAS patients revealed significantly higher median sBT levels in nc-MCAS patients, suggesting that the mere presence of extra copies of\n","the α- tryptase gene allele may not be sufficient to induce hypertryptasemia in healthy individuals without a history of anaphylaxis and allergic\n","diseases. These findings could influence the diagnostic criteria and management of nc-MCAS, potentially leading to earlier identification and\n","treatment of those with elevated sBT levels. However, it's crucial to consider other factors such as symptoms, histology, and molecular testing\n","results when making a definitive diagnosis. Additionally, the study also reported that anaphylaxis was more frequent in HAT+ versus HAT- mastocytosis\n","patients, which could further contribute to the differential diagnosis between these conditions.\n","\n","\n","Q24: How does the bidirectional state space model in Vim improve memory efficiency and computation speed compared to traditional vision transformers when handling high-resolution images?\n"," response: *The bidirectional state space model (SSM) in Vim improves memory efficiency and computation speed by enabling direct sequential visual\n","representation learning without relying on external 2D priors for high-resolution visual understanding tasks. This approach allows Vim to achieve\n","higher accuracy than traditional vision transformers like DeiT while saving significant GPU memory and reducing inference time.*\n","\n","In other words, Vim's bidirectional SSM enables the model to capture both past and future context in the input sequence, making it more effective for\n","handling long-range dependencies present in high-resolution images. By processing the visual sequence in both directions, Vim can learn more robust\n","representations, leading to better performance on various computer vision tasks. Additionally, since Vim doesn't rely on external 2D priors, it\n","requires less memory and computational resources compared to traditional vision transformers.\n","\n","For instance, according to the given paper, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-\n","resolution images. These improvements demonstrate Vim's potential to become the next-generation backbone for vision foundation models.\n","\n","\n","Q25: In what ways does the proposed Vision Mamba model overcome the challenges of position-sensitivity and the requirement of global context in visual data representation without relying on self-attention mechanisms?\n"," response: 2.1 Vision Mamba (Vim) addresses the challenges of position sensitivity and global context requirements in visual data representation through\n","bidirectional selective state space models (SSMs) and position embeddings. By splitting the input image into patches, linearly projecting them as\n","vectors, and treating image patches as sequence data in Vim blocks, Vim efficiently compresses the visual representation while enabling data-dependent\n","global visual context modeling. Position embeddings within Vim blocks provide awareness for spatial information, making Vim more robust in dense\n","prediction tasks. This approach allows Vim to achieve superior performance compared to plain vision transformers like DeiT, without the need for self-\n","attention mechanisms.\n","\n","\n","Q26: What architectural modifications and hyperparameters were employed in Vim to align its model sizes with those of DeiT series while ensuring efficiency in visual tasks?\n"," response: 2.8× faster, 86.8% less memory: Vim's architecture includes linear scaling in sequence length, leading to improved efficiency in high-resolution\n","downstream vision applications and long-sequence multimodal tasks compared to DeiT. However, specific architectural modifications and hyperparameter\n","tunings for Vim to match DeiT's model sizes aren't explicitly stated in the provided text. Instead, the text focuses on Vim's performance advantages\n","in terms of speed and memory usage relative to DeiT.\n","\n","\n","Q27: How does Vim perform in downstream dense prediction tasks, such as semantic segmentation and object detection, compared to traditional models?\n"," response: 2.8 times faster and saves 86.8% GPU memory than DeiT in high-resolution semantic segmentation tasks. Vim also achieves similar segmentation\n","performance as ResNet-101 but with nearly half the parameters. Similarly, in object detection tasks, Vim shows improved efficiency compared to other\n","SSM-based models.\n","\n","\n","Q28: How does the proposed amygdala-anterior hippocampal pathway for neurofibrillary tangle (NFT) spread challenge or complement the classical model of NFT propagation from the entorhinal cortex to the hippocampus, and what implications does this have for understanding the heterogeneity of Alzheimer’s disease progression?\n"," response: 2. The proposed amygdala-anterior hippocampal pathway for NFT spread challenges the classical model by suggesting an alternate source of NFT\n","pathology in the amygdala, potentially preceding the involvement of the entorhinal cortex and hippocampus. This implies that the heterogeneity of\n","Alzheimer's disease progression may involve both the amygdala and entorhinal cortex as independent or interconnected sources of NFT spread.\n","Complementarily, the existence of this pathway highlights the complexity of NFT propagation and underscores the importance of considering multiple\n","neural routes in understanding the progression of Alzheimer's disease.\n","\n","\n","Q29: How do the connectivity patterns and functional roles of different amygdala nuclei contribute to specific neuropsychiatric symptoms observed in Alzheimer’s patients, and what potential does this understanding have for developing targeted interventions to mitigate these symptoms?\n"," response: 2 The amygdala's complex network structure, consisting of various nuclei, plays a crucial role in processing emotions and memories related to\n","fear, anxiety, anger, and depression. In Alzheimer's disease, neuroimaging studies suggest that the amygdala may be involved in neuropsychiatric\n","symptoms, particularly in the disease's earliest stages. These symptoms include paranoia, anxiety, apathy, and aggression. Anatomical evidence\n","indicates that abnormal tau protein spread likely follows neural connections, suggesting that the amygdala's interconnected networks could serve as\n","conduits for Alzheimer's progression. Understanding the unique functions of different amygdala nuclei and their associated networks can potentially\n","lead to targeted interventions aimed at managing neuropsychiatric symptoms in Alzheimer's patients. However, further research is needed to validate\n","these findings and develop effective therapeutic strategies.\n","\n","\n","Q30: How does the discovery of the preferential accumulation of neurofibrillary tangles (NFTs) in the inferior-medial domain of the amygdala, supported by novel human data and high-resolution 3D reconstructions, impact our understanding of early Alzheimer's disease pathology, and what are the implications for early diagnosis and intervention?\n"," response: 2 The discovery of NFTs preferentially accumulating in the inferior-medial domain of the amygdala, as reported by Yushkevich et al and Stouffer et\n","al, provides crucial insights into the earliest stages of Alzheimer's disease pathology. This finding supports the notion that the amygdala plays a\n","significant role in Alzheimer's disease, especially during its onset when cognitive and neuropsychiatric symptoms emerge. By continuing to investigate\n","the amygdala through clinical imaging methods like MRI, researchers can better understand and monitor patients' progressing Alzheimer's disease\n","symptomatology. Additionally, this knowledge may eventually lead to predictive capabilities for diagnosing and intervening earlier in the disease\n","process.\n","\n","Furthermore, the evidence suggesting the amygdala's involvement in Alzheimer's disease, particularly in its earliest stages, highlights the importance\n","of studying this region in detail. The connectionist hypothesis proposes that NFT pathology spreads through strongly connected brain regions, and\n","recent human studies using functional MRI (fMRI) support this theory. The amygdala, which was previously underinvestigated compared to other regions\n","like the TER, ERC, and hippocampus, now receives renewed attention due to these novel imaging technologies and analytical techniques. Ultimately, this\n","increased understanding of the amygdala's role in Alzheimer's disease could pave the way for improved diagnostic tools and potential therapeutic\n","interventions.\n","\n","\n","Q31: What role does the proposed amygdala-anterior hippocampal pathway play in the occurrence of early neuropsychiatric symptoms in Alzheimer’s patients, and how might this new understanding influence future research directions and therapeutic approaches?\n"," response: 29The amygdala-anterior hippocampal pathway plays a crucial role in the occurrence of early neuropsychiatric symptoms in Alzheimer's patients.\n","Early Neurofibrillary Tangle (NFT) spread between the amygdala and anterior hippocampus could contribute to the 'neuropsychiatric-symptoms-first'\n","subtype of Alzheimer's disease. This finding highlights the importance of investigating the role of the amygdala in the spread of NFT throughout the\n","Medial Temporal Lobe (MTL), as well as its connection to observed clinical symptoms. Future research should aim to establish a definitive link between\n","amygdala involvement and specific neuropsychiatric symptoms, which could lead to novel therapeutic targets for treating Alzheimer's disease.\n","Additionally, understanding the underlying mechanisms of this pathway could shed light on the neurobiological basis of neuropsychiatric symptoms in\n","Alzheimer's disease and potentially inform the development of personalized treatment strategies.\n","\n","\n","Q32: How do selective state space models improve content-based reasoning in sequence modeling compared to traditional architectures?\n"," response:  Great question! Selective state space models (SMSMs) improve content-based reasoning in sequence modeling by introducing a selection mechanism\n","that allows the model to focus on or filter out specific inputs based on their relevance to the current context. This contrasts with traditional\n","architectures, which often rely solely on fixed kernels or recurrent interactions to process input sequences. By enabling context-awareness through\n","selectivity, SMSMs can achieve better compression of necessary information and improved efficiency without compromising effectiveness. Additionally,\n","SMSMs offer potential applications in various domains, including genomics, audio, and video, making them promising candidates for foundation models.\n","\n","\n","Q33: What are the advantages of Mamba's hardware-aware parallel algorithm in recurrent mode over traditional convolution-based methods?\n"," response: 1. Lower Floating Point Operations (FLOPs): Although recurrent computations require more FLOPs per step compared to convolutional ones, they have\n","a smaller constant factor. Therefore, for long sequences and moderate state dimensions, recurrent mode can consume fewer FLOPs overall.\n","\n","2. Addresses Large Memory Usage: One challenge with recurrent networks is their sequential nature and large memory requirements. Mamba's hardware-\n","aware parallel algorithm employs techniques like kernel fusion, parallel scan, and recomputation to mitigate this issue. These techniques enable\n","efficient processing and linear scaling in sequence length.\n","\n","3. Vectorization and Convolution Representation: Mamba converts the recurrent state equation into a vectorized form, which can then be represented as\n","a single (non-circular) convolution. This representation allows for efficient computation using Fast Fourier Transforms (FFTs), making the process\n","significantly faster. However, calculating the convolution kernel (filter) directly from the equation is non-trivial and is modeled as an SSM\n","convolution kernel or filter.\n","\n","In summary, Mamba's hardware-aware parallel algorithm in recurrent mode offers advantages such as lower FLOP count, effective handling of large memory\n","usage, and efficient vectorization and convolution representation.\n","\n","\n","Q34: In what ways does Mamba achieve better performance across modalities such as language, audio, and genomics compared to Transformers of similar sizes?\n"," response: 1. Selective attention mechanism: Mamba's selectivity brings stronger performance on dense modalities like language, audio, and genomics compared\n","to Transformers due to its ability to focus on relevant features.\n","2. Fast training and inference: Mamba's computational requirements and memory usage scale linearly with sequence length during training. During\n","inference, its autoregressive unrolling method requires only constant time per step, as it doesn't need a cache of previous elements.\n","3. Long context handling: Mamba's superior quality and efficiency together lead to performance improvements on real data up to sequence lengths of 1M.\n","4. Superior performance on synthetics: Mamba excels at solving important synthetic tasks, such as copying and induction heads, which are considered\n","crucial for large language models. It can handle these tasks efficiently even with long sequences (>1M tokens).\n","5. Comparable memory footprint: As shown in Table 15, Mamba's memory requirement is comparable to a similarly-sized Transformer with an optimized\n","implementation. This allows Mamba to process larger datasets with minimal resource constraints.\n","\n","\n","Q35: How does Mamba handle long sequences efficiently, and what benefits does this bring to real-world applications?\n"," response: 1. Mamba handles long sequences efficiently through its linear scalability in sequence length during training and fast autoregressive inference.\n","This means that the quality and efficiency of Mamba's contextual reasoning remain consistent even for extremely long sequences.\n","2. In real-world applications, Mamba's ability to process long sequences leads to improved performance on various modalities and settings. These\n","include synthetics like copying and induction tasks, which Mamba can solve easily and extrapolate solutions to lengths greater than one million\n","tokens. Additionally, Mamba shows promising results in emerging modalities such as genomics, audio, and video, making it a strong candidate to serve\n","as a general sequence model backbone.\n","\n","\n","Q36: How does the Joint Medical LLM and Retrieval Training (JMLR) approach reduce hallucinations in medical question-answering tasks?\n"," response: 2. The JMLR approach reduces hallucinations in medical question-answering tasks by allowing the model to consult different medical guidelines and\n","retrieve corresponding answers and explanations. This alignment with real-world medical information increases the accuracy and credibility of the\n","model's explanations, thereby reducing the occurrence of factually incorrect outputs. Additionally, the synchronization of LLM and IR training during\n","the fine-tuning phase enhances their ability to work together, reducing the demand for computational resources and improving overall performance.\n","\n","\n","Q37: Why does the JMLR model require less computational resources compared to traditional pretraining methods for medical language models?\n"," response:  Great question! The JMLR model's reduced computational resource requirement comes from its joint training approach. By integrating information\n","retrieval (IR) and large language model (LLM) training, it efficiently utilizes existing knowledge rather than requiring extensive pretraining.\n","Additionally, the use of external data resources like MIMIC-IV, medical textbooks, and diverse medical documents further expands the model's knowledge\n","base. This combination leads to significant improvements in medical question answering, as shown in Figure 1, where JMLR achieves higher accuracy\n","compared to other models like PMC-Llama, Meditron, and ChatGPT.\n","\n","\n","Q38: In what way does JMLR improve the accuracy of medical question-answering over traditional Retrieval-Augmented Generation (RAG) methods?\n"," response:  JMLR improves medical question-answering accuracy over RAG methods by integrating language model training and retrieval training in a joint\n","manner. This approach allows the system to utilize external knowledge during inference, enhancing its ability to provide accurate and specific\n","answers, particularly in complex medical scenarios. JMLR outperformed Meditron-70B and Llama2-13B with RAG on a medical question-answering dataset,\n","achieving higher accuracy with fewer computational resources.\n","\n","\n","Q39: How does the JMLR model handle the challenge of providing detailed reasoning for its answers in medical question-answering tasks?\n"," response: 2 The JMLR model improves its reasoning abilities by combining joint medical LLM and retrieval training. It retrieves related cases and uses them\n","to offer comparable treatment options, thereby enhancing decision-making processes. In evaluations against Amboss and MedQA test sets, JMLR's\n","generated reasoning was deemed superior by both GPT-4 and medical professionals. This integration led to substantial performance improvements across\n","multiple medical benchmarks. However, it's important to note that the application of this method to other domains remains unexplored.\n","\n","\n","Q40: How can training a language model with a mix of relevant and distractor documents improve its performance in an open-book exam setting?\n"," response:  By incorporating both relevant and distractor documents during training, a language model becomes adept at distinguishing pertinent information\n","from irrelevant ones, enhancing its ability to identify and utilize relevant content efficiently in an open-book exam setting. This strategy improves\n","the model's robustness against irrelevant texts in retrieved documents.\n","\n","\n","Q41: What are the benefits of incorporating chain-of-thought reasoning in the training process of language models?\n"," response: 1. Improved overall accuracy: By guiding the model to the answer and enriching its understanding through a reasoning chain, chain-of-thought\n","reasoning significantly enhances the training robustness. (Refer to Jason Wei et al., 2022)\n","2. Clear source citations: Integrating chain-of-thought reasoning allows the model to form a reasoning chain that appropriately references the\n","original context, which enhances its accuracy by providing clear source citations. (Refer to RAFT, 2023)\n","3. Better handling of complex queries: Chain-of-thought reasoning enables language models to handle complex queries by breaking them down into smaller\n","steps and providing a more comprehensive understanding of the context. (Implied from the text)\n","4. Enhanced ability to reason and generalize: By encouraging language models to think through their answers and reference the original context, chain-\n","of-thought reasoning improves their ability to reason and generalize across different domains and tasks. (Implied from the text)\n","5. Increased transparency and interpretability: The use of chain-of-thought reasoning makes the language model's decision-making process more\n","transparent and interpretable, allowing researchers and developers to better understand how the model arrives at its answers. (Implied from the text)\n","\n","\n","Q42: How does the RAFT methodology ensure robustness against inaccurate document retrieval during test time?\n"," response: 1. The RAFT methodology trains models to understand the dynamics between the question, retrieved documents, and the appropriate answer, making\n","them more resilient to fluctuations in the number of documents encountered during testing.\n","2. By including distractor documents during training, the model becomes better prepared for a range of scenarios it may encounter in real-world\n","applications, further validating its robustness.\n","3. Existing fine-tuning based approaches either don't utilize documents at test time or fail to account for imperfections in the retrieval process\n","during training, which RAFT addresses by combining supervised fine-tuning and retrieval augmented generation.\n","\n","\n","Q43: Why is it sometimes beneficial to exclude the oracle document during the training of language models for domain-specific tasks?\n"," response:  Great question! According to the text you've provided, it suggests that excluding the oracle document during the training of language models for\n","domain-specific Retrieval-Augmented Generation (RAG) tasks can actually enhance the model's performance. This is because the model may learn to\n","identify and utilize relevant information even when the oracle document isn't present. However, it's important to note that the optimal proportion of\n","training instances without the oracle document varies across datasets, so finding the right balance is crucial. Additionally, including some\n","irrelevant documents alongside the oracle document during training can help improve the model's ability to distinguish and ignore irrelevant\n","information. Overall, these strategies aim to strengthen the model's efficiency in identifying and utilizing relevant information for domain-specific\n","RAG tasks.\n","\n","\n"]}]},{"cell_type":"code","source":["json_file_name = 'QA_dvancedMedicalAI.json'\n","df_json = pd.DataFrame([Q_dictionary])\n","df_json.to_json(json_file_name, orient='records', lines=True)"],"metadata":{"id":"w9jTIP0_Aruq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["csv_file_name = 'QA_dvancedMedicalAI.csv'\n","df_csv = pd.DataFrame([Q_dictionary])\n","df_csv.to_csv(csv_file_name, index=False)"],"metadata":{"id":"USjnu7lKAryv"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"83be7aa6f3344d81b8ad8c4e5ad7e136":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_3c4990652e534814945014d27f2e89ea","IPY_MODEL_41dea8cd57724383bd7d40cd9bfcc564","IPY_MODEL_a8ba9fe549734470b0ce991158d23730","IPY_MODEL_abe1515d56b441eb95decce85907fb88"],"layout":"IPY_MODEL_0311b7aeec7842b49eb435175fdda52e"}},"2b8fc2e50d804cba8eb3368775eb50a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_309bf044781f4a10b91a6e798404a2b1","placeholder":"​","style":"IPY_MODEL_76907780c959474ea1454479fdf1817e","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"e13d180bac5445b8858cf5c80ce0f718":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_df8924d3eeb9443987b5bb4a0e337559","placeholder":"​","style":"IPY_MODEL_d7c3cffa16724dc4b6c52286b0934be7","value":""}},"4058760247e74907a04b8729389d6328":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_f0e15ef9bd064c07a3bc953ce33c01c3","style":"IPY_MODEL_450cd70095d04eea8517c5fe166c67e0","value":true}},"52d5e6bf4dc0490daa370ccd992000c8":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_63402e4d5703412b8316bd60b2e5757d","style":"IPY_MODEL_2cc5daf7a2af4d8ba7314db1ebf81b8b","tooltip":""}},"75d53e90c9814be0aeee54d75d89a918":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_524e828f7231495aa6ee3afb121290db","placeholder":"​","style":"IPY_MODEL_cfde14930ec24b608202eb3cc7595f9c","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"0311b7aeec7842b49eb435175fdda52e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"309bf044781f4a10b91a6e798404a2b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76907780c959474ea1454479fdf1817e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df8924d3eeb9443987b5bb4a0e337559":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7c3cffa16724dc4b6c52286b0934be7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0e15ef9bd064c07a3bc953ce33c01c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"450cd70095d04eea8517c5fe166c67e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63402e4d5703412b8316bd60b2e5757d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cc5daf7a2af4d8ba7314db1ebf81b8b":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"524e828f7231495aa6ee3afb121290db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfde14930ec24b608202eb3cc7595f9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c8fa1e78df34b25a6308c6b5bdc26af":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e11b6624a580435bb7127b36035ad4a3","placeholder":"​","style":"IPY_MODEL_3e13dd3187504ee49be7ffe39ddf4caa","value":"Connecting..."}},"e11b6624a580435bb7127b36035ad4a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e13dd3187504ee49be7ffe39ddf4caa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c4990652e534814945014d27f2e89ea":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_701ab428cb684773a4f966c722490cdf","placeholder":"​","style":"IPY_MODEL_3daeae51af814438913dc81c9d3247de","value":"Token is valid (permission: write)."}},"41dea8cd57724383bd7d40cd9bfcc564":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_055acc58e5314a6a9e5f4cc032baf737","placeholder":"​","style":"IPY_MODEL_d000e9b998f548069b17106341dea55d","value":"Your token has been saved in your configured git credential helpers (store)."}},"a8ba9fe549734470b0ce991158d23730":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec954ed541574533a83364ca0bbac30c","placeholder":"​","style":"IPY_MODEL_1cf662e808824297909f2ce419d86148","value":"Your token has been saved to /root/.cache/huggingface/token"}},"abe1515d56b441eb95decce85907fb88":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0b1aad197aa492894a9411323c50dab","placeholder":"​","style":"IPY_MODEL_3bc4d72416fe427796684173688c805d","value":"Login successful"}},"701ab428cb684773a4f966c722490cdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3daeae51af814438913dc81c9d3247de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"055acc58e5314a6a9e5f4cc032baf737":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d000e9b998f548069b17106341dea55d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec954ed541574533a83364ca0bbac30c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cf662e808824297909f2ce419d86148":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0b1aad197aa492894a9411323c50dab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bc4d72416fe427796684173688c805d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06706cb58b0e45308a8b9308da3ad9e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3a8a355c1de54d70bc02db03f3d143de","IPY_MODEL_5eb2c25d8e0e4191a218577a50eb8b0e","IPY_MODEL_9472445b06434513b1cfba31cdea6d65"],"layout":"IPY_MODEL_c55efce0462a4d43a221b90776bb4cfb"}},"3a8a355c1de54d70bc02db03f3d143de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69655b69b90040629033ea37da54369d","placeholder":"​","style":"IPY_MODEL_f5995cd08cf24890ba696057de05aa74","value":"tokenizer_config.json: 100%"}},"5eb2c25d8e0e4191a218577a50eb8b0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_601d1aadd78d444ab6b1cf59eab557db","max":1460,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ff4f6ec0290451b9258fc085ead19c5","value":1460}},"9472445b06434513b1cfba31cdea6d65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ab3eb5f6a314e38a7299a1e5f07b9a6","placeholder":"​","style":"IPY_MODEL_295e0e7f15ec4d1a8c3a56a66025d3ee","value":" 1.46k/1.46k [00:00&lt;00:00, 127kB/s]"}},"c55efce0462a4d43a221b90776bb4cfb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69655b69b90040629033ea37da54369d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5995cd08cf24890ba696057de05aa74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"601d1aadd78d444ab6b1cf59eab557db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ff4f6ec0290451b9258fc085ead19c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ab3eb5f6a314e38a7299a1e5f07b9a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"295e0e7f15ec4d1a8c3a56a66025d3ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8fcd1f42ef64588835e88075527292b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_05c659300e054498ba341fae2afb98bd","IPY_MODEL_8c4573c889fc4655b464e6a269c155a2","IPY_MODEL_914322c4fa54422b8502cd51b6f9c4f0"],"layout":"IPY_MODEL_ba57826641444374a50f5a45039b922e"}},"05c659300e054498ba341fae2afb98bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8211887d9cc41d88787f28175592b5c","placeholder":"​","style":"IPY_MODEL_d51559af1d8a47428140513eb483b8da","value":"tokenizer.model: 100%"}},"8c4573c889fc4655b464e6a269c155a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_50ae5c800a9d4f78856cece638eb40e8","max":493443,"min":0,"orientation":"horizontal","style":"IPY_MODEL_52b6e771814e4b5ca97ca7925d70e13a","value":493443}},"914322c4fa54422b8502cd51b6f9c4f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70f2dc2066384d62948d4c220a7c900b","placeholder":"​","style":"IPY_MODEL_491d954628214bad91dde80ca8c0d8a0","value":" 493k/493k [00:00&lt;00:00, 5.87MB/s]"}},"ba57826641444374a50f5a45039b922e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8211887d9cc41d88787f28175592b5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d51559af1d8a47428140513eb483b8da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50ae5c800a9d4f78856cece638eb40e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52b6e771814e4b5ca97ca7925d70e13a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"70f2dc2066384d62948d4c220a7c900b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"491d954628214bad91dde80ca8c0d8a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f3f73c5b06648c6945c81915c6e9308":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed026d8c163d49e6a89a76efa763dfb7","IPY_MODEL_9a41777a1d7e4ac78aa5fcdcf38e253e","IPY_MODEL_0f5624bf2d2b498290aa2d7ec014e342"],"layout":"IPY_MODEL_ef6095bb224c47488a741cc8a7940f8f"}},"ed026d8c163d49e6a89a76efa763dfb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6b68f6fa9f34a9cb75ee12396143052","placeholder":"​","style":"IPY_MODEL_b126a40185c64757b71c1492912ce9e2","value":"tokenizer.json: 100%"}},"9a41777a1d7e4ac78aa5fcdcf38e253e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc49e24b62154e43a91311826530cfaf","max":1795303,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b52723d8649240e7ac4e5d339200e6c0","value":1795303}},"0f5624bf2d2b498290aa2d7ec014e342":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_259b7322eaca44e387574cf366bedfca","placeholder":"​","style":"IPY_MODEL_0b96ddd6145b48189760119f5c6b4632","value":" 1.80M/1.80M [00:00&lt;00:00, 7.98MB/s]"}},"ef6095bb224c47488a741cc8a7940f8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6b68f6fa9f34a9cb75ee12396143052":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b126a40185c64757b71c1492912ce9e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc49e24b62154e43a91311826530cfaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b52723d8649240e7ac4e5d339200e6c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"259b7322eaca44e387574cf366bedfca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b96ddd6145b48189760119f5c6b4632":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0cef02e719a04fa2ba3d3a82a39a0a5e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_786b638d187b41fca8af1d8932333878","IPY_MODEL_ac77addcaaeb406da998f1066135937a","IPY_MODEL_91b690de32af47baa5660bf0fc7bae90"],"layout":"IPY_MODEL_02323dbb179c4431a4c4ba6314199cb0"}},"786b638d187b41fca8af1d8932333878":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acb34be0f8ab46a78f03775337c4e9db","placeholder":"​","style":"IPY_MODEL_bafbf758b9864d28aac252fc635c8ce4","value":"special_tokens_map.json: 100%"}},"ac77addcaaeb406da998f1066135937a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06e90fd0f23342d7b148b144e0a54809","max":72,"min":0,"orientation":"horizontal","style":"IPY_MODEL_876dea86836c4d5d96f09a3ba7e343c5","value":72}},"91b690de32af47baa5660bf0fc7bae90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a562d13bacca4494a307b0c655f3943b","placeholder":"​","style":"IPY_MODEL_5f92206b6dd3488c9796254e8b2bac7d","value":" 72.0/72.0 [00:00&lt;00:00, 6.75kB/s]"}},"02323dbb179c4431a4c4ba6314199cb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acb34be0f8ab46a78f03775337c4e9db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bafbf758b9864d28aac252fc635c8ce4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06e90fd0f23342d7b148b144e0a54809":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"876dea86836c4d5d96f09a3ba7e343c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a562d13bacca4494a307b0c655f3943b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f92206b6dd3488c9796254e8b2bac7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce4e0277f9c9498c872f0549e8cfb0e1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_06c135caef9642de9e322a05a9376798","IPY_MODEL_103dd651f1524c23b1e0b9cca25846d8","IPY_MODEL_af2fddb2d5f74515b3d095e2f9341433"],"layout":"IPY_MODEL_f03961e9cc5a4e198edd35dbffeffe9d"}},"06c135caef9642de9e322a05a9376798":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9a3d23179a545998a10a42a99dc9c39","placeholder":"​","style":"IPY_MODEL_7cd8a7abbbdc4670a052db70c37596d2","value":"config.json: 100%"}},"103dd651f1524c23b1e0b9cca25846d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56934cde1bd44788844657fe5801f6e0","max":596,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb97f541a68f48278204fb2a285a092d","value":596}},"af2fddb2d5f74515b3d095e2f9341433":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0592760a6d9a47d9b8c92884a8245f32","placeholder":"​","style":"IPY_MODEL_b6203695efdc40fb82d82301bf03d564","value":" 596/596 [00:00&lt;00:00, 51.8kB/s]"}},"f03961e9cc5a4e198edd35dbffeffe9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9a3d23179a545998a10a42a99dc9c39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cd8a7abbbdc4670a052db70c37596d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56934cde1bd44788844657fe5801f6e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb97f541a68f48278204fb2a285a092d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0592760a6d9a47d9b8c92884a8245f32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6203695efdc40fb82d82301bf03d564":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3fa7e54443b47a2b62a83128b98d990":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cf4f4c8473b44f3ab5b7ca7f5361e89d","IPY_MODEL_c1455e5a37594a7b965374dabe7f3e04","IPY_MODEL_f8c45a0102c843589e9d13f5de79c0bc"],"layout":"IPY_MODEL_881504ac870a4545b81cffabf46a127d"}},"cf4f4c8473b44f3ab5b7ca7f5361e89d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53ab111c9e5d4541a77751fe2cad6524","placeholder":"​","style":"IPY_MODEL_c1caf176f8b84301967f164b373adf5a","value":"model.safetensors.index.json: 100%"}},"c1455e5a37594a7b965374dabe7f3e04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_df2cdb18047547d48c83172ec3bd89bd","max":25125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e9bf614a16ec4332b774e0a163d00fcb","value":25125}},"f8c45a0102c843589e9d13f5de79c0bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d058d8670044e118e54cc8acc9414da","placeholder":"​","style":"IPY_MODEL_06fdb6bd0acb4b35a2c7c6809d4834d8","value":" 25.1k/25.1k [00:00&lt;00:00, 2.20MB/s]"}},"881504ac870a4545b81cffabf46a127d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53ab111c9e5d4541a77751fe2cad6524":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1caf176f8b84301967f164b373adf5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df2cdb18047547d48c83172ec3bd89bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9bf614a16ec4332b774e0a163d00fcb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d058d8670044e118e54cc8acc9414da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06fdb6bd0acb4b35a2c7c6809d4834d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cbb6dec2a2740e6b387b4f71618156a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47ee4856f118412caab8f940f1f0ae3f","IPY_MODEL_24a7ff75918e4e23bf107328e28e9f52","IPY_MODEL_220b68131e304a7b979df51fd41aa35f"],"layout":"IPY_MODEL_1f495fd897fc4b85bbf8aa90d9dc11b8"}},"47ee4856f118412caab8f940f1f0ae3f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0899e1400f184c519cf16d43192477d8","placeholder":"​","style":"IPY_MODEL_a10bfddfdbd3489e9d381306487994d0","value":"Downloading shards: 100%"}},"24a7ff75918e4e23bf107328e28e9f52":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_038d4d615f8b4dcab84b987d274c6a9c","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eee0ee55326245b6812da534a369ecfb","value":3}},"220b68131e304a7b979df51fd41aa35f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a130bf9aae5470586a0c3eaba589f27","placeholder":"​","style":"IPY_MODEL_0ce6c558e19f432f90b8055ba2d12fb4","value":" 3/3 [01:41&lt;00:00, 33.04s/it]"}},"1f495fd897fc4b85bbf8aa90d9dc11b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0899e1400f184c519cf16d43192477d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a10bfddfdbd3489e9d381306487994d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"038d4d615f8b4dcab84b987d274c6a9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eee0ee55326245b6812da534a369ecfb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a130bf9aae5470586a0c3eaba589f27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ce6c558e19f432f90b8055ba2d12fb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e49553d53614a18a365e3f7dcab9b31":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_98717ece4417433d9b623182a50eeb94","IPY_MODEL_2582ba3c3fc745428a1cb1e836b6d8ae","IPY_MODEL_b6645a693a2a4776bda2cb80a022195d"],"layout":"IPY_MODEL_956b0ed0794141b59c2483db257d3237"}},"98717ece4417433d9b623182a50eeb94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ed3a1d974ff41d2b0da4300674d4ae4","placeholder":"​","style":"IPY_MODEL_8983b8243d514e3ba30e468eff624a8a","value":"model-00001-of-00003.safetensors: 100%"}},"2582ba3c3fc745428a1cb1e836b6d8ae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44b75b1ef0e44e83b66496dd0b8697ff","max":4943162336,"min":0,"orientation":"horizontal","style":"IPY_MODEL_21e5f711e0e04cf6ae04ba75729c87bf","value":4943162336}},"b6645a693a2a4776bda2cb80a022195d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68328d064a774b2bad4c8fba8c63d9cb","placeholder":"​","style":"IPY_MODEL_7c49beb9e44e4406a9a94e29685b7602","value":" 4.94G/4.94G [00:35&lt;00:00, 82.4MB/s]"}},"956b0ed0794141b59c2483db257d3237":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ed3a1d974ff41d2b0da4300674d4ae4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8983b8243d514e3ba30e468eff624a8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44b75b1ef0e44e83b66496dd0b8697ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21e5f711e0e04cf6ae04ba75729c87bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68328d064a774b2bad4c8fba8c63d9cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c49beb9e44e4406a9a94e29685b7602":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90eee8f51b1e49f791f380268b155b1a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6c99e52b564f4f7d9792ec89ac3d711e","IPY_MODEL_a3a8ed9e1d2d42a5b9db1f8f253f2fc6","IPY_MODEL_cfc4b9823f5d4ce880c8b1385ee94939"],"layout":"IPY_MODEL_552c4ffe362d4d4290cfb5b0b2054a85"}},"6c99e52b564f4f7d9792ec89ac3d711e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77b9806b57ad408691f05e6fd367a3fd","placeholder":"​","style":"IPY_MODEL_74378a405fc647b293ef2ffe5b0737ac","value":"model-00002-of-00003.safetensors: 100%"}},"a3a8ed9e1d2d42a5b9db1f8f253f2fc6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8800ac3e2db4412cbde6b23e53f5a143","max":4999819336,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac7c98b9aa444f03bfc4d00cfa0f534d","value":4999819336}},"cfc4b9823f5d4ce880c8b1385ee94939":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f749d881390448aa826b1b489609bdb7","placeholder":"​","style":"IPY_MODEL_78168884f61f4ac9bf7f83a5f07f0f09","value":" 5.00G/5.00G [00:35&lt;00:00, 155MB/s]"}},"552c4ffe362d4d4290cfb5b0b2054a85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77b9806b57ad408691f05e6fd367a3fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74378a405fc647b293ef2ffe5b0737ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8800ac3e2db4412cbde6b23e53f5a143":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac7c98b9aa444f03bfc4d00cfa0f534d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f749d881390448aa826b1b489609bdb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78168884f61f4ac9bf7f83a5f07f0f09":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"298ace6d5e63469895288e4084b006ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c28df59f6a944a29be3251bd867e4b17","IPY_MODEL_87efa7dc0f904ddd8d76dbea743736e3","IPY_MODEL_5c3419023ecf4214867cf8cf67babb06"],"layout":"IPY_MODEL_10c6088f22ad412e968ce1aea13e847e"}},"c28df59f6a944a29be3251bd867e4b17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_190cdb0b0e09408b8dfbd5648830e5e7","placeholder":"​","style":"IPY_MODEL_77f9e5643ec2496f9580e360099e582d","value":"model-00003-of-00003.safetensors: 100%"}},"87efa7dc0f904ddd8d76dbea743736e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de4a409116254463acce09b9ec9dec95","max":4540516344,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1f29660e99864f53bd11610771b7d10a","value":4540516344}},"5c3419023ecf4214867cf8cf67babb06":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_521f42d4bc0c4a9ca525db552bf07e81","placeholder":"​","style":"IPY_MODEL_28bba7700b6b4aee88c8ad1647d6437d","value":" 4.54G/4.54G [00:29&lt;00:00, 106MB/s]"}},"10c6088f22ad412e968ce1aea13e847e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"190cdb0b0e09408b8dfbd5648830e5e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77f9e5643ec2496f9580e360099e582d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de4a409116254463acce09b9ec9dec95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f29660e99864f53bd11610771b7d10a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"521f42d4bc0c4a9ca525db552bf07e81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28bba7700b6b4aee88c8ad1647d6437d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5278baa3cc09475f9bb07b4d41d57728":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83863777f6ec412b9f8639914694efd7","IPY_MODEL_ba46dd93b4ac454b884fea8b0ca34523","IPY_MODEL_c8a4a5b924f84540b3ab57c95db495af"],"layout":"IPY_MODEL_1b53b8d75669428786559079914b3132"}},"83863777f6ec412b9f8639914694efd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8eb1cbdba6204371a10a0882aff8b68a","placeholder":"​","style":"IPY_MODEL_41e3eac9368546f8953f147962936278","value":"Loading checkpoint shards: 100%"}},"ba46dd93b4ac454b884fea8b0ca34523":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c33940b6b2694808b9ec5b77aa755337","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c67df4dd9a5e4e3a948c6b1442bfe715","value":3}},"c8a4a5b924f84540b3ab57c95db495af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42a3f7a7ff1a494192966ae0d18d176e","placeholder":"​","style":"IPY_MODEL_c361b7cd5d164018b62c977b6e991269","value":" 3/3 [00:08&lt;00:00,  2.79s/it]"}},"1b53b8d75669428786559079914b3132":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8eb1cbdba6204371a10a0882aff8b68a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41e3eac9368546f8953f147962936278":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c33940b6b2694808b9ec5b77aa755337":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c67df4dd9a5e4e3a948c6b1442bfe715":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"42a3f7a7ff1a494192966ae0d18d176e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c361b7cd5d164018b62c977b6e991269":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71d9c9084c7c488e8e8054948dd175af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_64d88a5f7f7940d6a23e8a3311469cd5","IPY_MODEL_bb1146d51a104a33aa42148972616bd4","IPY_MODEL_f09357a79e3a4789942d5edad77ab3d9"],"layout":"IPY_MODEL_ce4ae1bf08f2471496e628257ebb817b"}},"64d88a5f7f7940d6a23e8a3311469cd5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4832976a9f34a1085e3203af5635309","placeholder":"​","style":"IPY_MODEL_b4a562e3421945f1b5a7772b921dcf04","value":"generation_config.json: 100%"}},"bb1146d51a104a33aa42148972616bd4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfd2732821d84460aa128b456ed25998","max":111,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0549289a02ad4678a4559b358927c029","value":111}},"f09357a79e3a4789942d5edad77ab3d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a740d4a884541bda0e0be69070ef636","placeholder":"​","style":"IPY_MODEL_ca42d55ff1794cb495be455b8658963f","value":" 111/111 [00:00&lt;00:00, 10.7kB/s]"}},"ce4ae1bf08f2471496e628257ebb817b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4832976a9f34a1085e3203af5635309":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4a562e3421945f1b5a7772b921dcf04":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bfd2732821d84460aa128b456ed25998":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0549289a02ad4678a4559b358927c029":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a740d4a884541bda0e0be69070ef636":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca42d55ff1794cb495be455b8658963f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa7f8598cdc8436b854d54440f74307e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b6ce4e3070941b3b02a6ed6681ba349","IPY_MODEL_b9b085c19f814d5abc51e6bd693145d5","IPY_MODEL_2a135b5f305e43d480d6bb74d48cdeca"],"layout":"IPY_MODEL_475544bad9f24e81bcb1493f054f64e5"}},"9b6ce4e3070941b3b02a6ed6681ba349":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db3aaba22c884e388716d7a193101d80","placeholder":"​","style":"IPY_MODEL_e7266f328fff4e0c8b1c99c2731a6286","value":"modules.json: 100%"}},"b9b085c19f814d5abc51e6bd693145d5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0178535c33540f89a9fd34cf0642756","max":349,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78c6509ffa484f77ac67fcafbca02f2a","value":349}},"2a135b5f305e43d480d6bb74d48cdeca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1964b39c8684b07acc060097fbf406f","placeholder":"​","style":"IPY_MODEL_f613a5f8fb64492b880ce26d1da522ae","value":" 349/349 [00:00&lt;00:00, 31.5kB/s]"}},"475544bad9f24e81bcb1493f054f64e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db3aaba22c884e388716d7a193101d80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7266f328fff4e0c8b1c99c2731a6286":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0178535c33540f89a9fd34cf0642756":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78c6509ffa484f77ac67fcafbca02f2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1964b39c8684b07acc060097fbf406f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f613a5f8fb64492b880ce26d1da522ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08d23fca9fd9460a927f890f81894199":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c0afc32da76a4e94a437ab6ab685b0b8","IPY_MODEL_fb8242d10a9845658d85dc87ec30eac1","IPY_MODEL_8fa10a987e854b97831d104bf10301b3"],"layout":"IPY_MODEL_895c67c123534d5eb6b3c00c8ef1d251"}},"c0afc32da76a4e94a437ab6ab685b0b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c498d2fac37b4649a9f5e1623ba78f55","placeholder":"​","style":"IPY_MODEL_43371bb779ca4edbb30a431d3f94381d","value":"config_sentence_transformers.json: 100%"}},"fb8242d10a9845658d85dc87ec30eac1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c097233e770945339b41dde3a5ad88fc","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36d8f43ea9c147059ffa64a59f062a82","value":124}},"8fa10a987e854b97831d104bf10301b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d12e0ebb763c49a09d82dd3fbcb7a5ff","placeholder":"​","style":"IPY_MODEL_f22a70183b0643bfa9c9fe6633856f5a","value":" 124/124 [00:00&lt;00:00, 11.1kB/s]"}},"895c67c123534d5eb6b3c00c8ef1d251":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c498d2fac37b4649a9f5e1623ba78f55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43371bb779ca4edbb30a431d3f94381d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c097233e770945339b41dde3a5ad88fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36d8f43ea9c147059ffa64a59f062a82":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d12e0ebb763c49a09d82dd3fbcb7a5ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f22a70183b0643bfa9c9fe6633856f5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29fdc46971ea4d1fbcb72f3e1866259e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_46b6fe67aebc45808f5908088912d6ef","IPY_MODEL_0b3c8d11eb17401e8d98229ba3bf5206","IPY_MODEL_3f4eb0fc48574db4b5bf090207097a15"],"layout":"IPY_MODEL_7bae81156a2342eca11c4ad99fdaeaa2"}},"46b6fe67aebc45808f5908088912d6ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d25c6f2e5da745afa38e87016ae88341","placeholder":"​","style":"IPY_MODEL_53234aaa710e483ca36f5b43d1cb093f","value":"README.md: 100%"}},"0b3c8d11eb17401e8d98229ba3bf5206":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_582e1f8f0bab45e590f01e75231b6ee5","max":94551,"min":0,"orientation":"horizontal","style":"IPY_MODEL_274b0f09966d4bc0b054173c1b4865de","value":94551}},"3f4eb0fc48574db4b5bf090207097a15":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7301dcbc8ff342389bf4bd907a11320e","placeholder":"​","style":"IPY_MODEL_c842aeac41394a42b7a175e2e1e11d43","value":" 94.6k/94.6k [00:00&lt;00:00, 3.65MB/s]"}},"7bae81156a2342eca11c4ad99fdaeaa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d25c6f2e5da745afa38e87016ae88341":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53234aaa710e483ca36f5b43d1cb093f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"582e1f8f0bab45e590f01e75231b6ee5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"274b0f09966d4bc0b054173c1b4865de":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7301dcbc8ff342389bf4bd907a11320e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c842aeac41394a42b7a175e2e1e11d43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d18426fe6afb4530b90ab3610c55e46c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c4f2fff67c07478387c5621aa76d5a01","IPY_MODEL_57852ec2b1bb4d8d90b9746f69296a09","IPY_MODEL_821e4f9dc3d645de9c0dbed50bda2c19"],"layout":"IPY_MODEL_6eae54b8242d4c2fbc3fc8f0239f9518"}},"c4f2fff67c07478387c5621aa76d5a01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bb8f0d30f7b42c4848eca00649d485a","placeholder":"​","style":"IPY_MODEL_887ff7b3de844b5bb06aff45184a746a","value":"sentence_bert_config.json: 100%"}},"57852ec2b1bb4d8d90b9746f69296a09":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_276efaaf44d24bac93bc649beeb9cd1d","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_092077b04cbb48dfbb37f616945d8e64","value":52}},"821e4f9dc3d645de9c0dbed50bda2c19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e45fb8f43d74801a00edece3702262e","placeholder":"​","style":"IPY_MODEL_74d540cf785944fdbfc631ba25b911ee","value":" 52.0/52.0 [00:00&lt;00:00, 3.90kB/s]"}},"6eae54b8242d4c2fbc3fc8f0239f9518":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bb8f0d30f7b42c4848eca00649d485a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"887ff7b3de844b5bb06aff45184a746a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"276efaaf44d24bac93bc649beeb9cd1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"092077b04cbb48dfbb37f616945d8e64":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e45fb8f43d74801a00edece3702262e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74d540cf785944fdbfc631ba25b911ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d04aca9e30f4c6e8a9cd6883e13955f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f25b47e90fdd460587d030c912716604","IPY_MODEL_a217c3b988b64c79a4414ae864761082","IPY_MODEL_04080a985fa34db29af8994f6a0bc194"],"layout":"IPY_MODEL_0475fb1047c8432e8390fe5313583a94"}},"f25b47e90fdd460587d030c912716604":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d01e9a266b440cb95424dcee3ed86cd","placeholder":"​","style":"IPY_MODEL_62e33de9223c4ea1b8da81e211a359eb","value":"config.json: 100%"}},"a217c3b988b64c79a4414ae864761082":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_573e2e0409d340a68134f7f9039ca982","max":777,"min":0,"orientation":"horizontal","style":"IPY_MODEL_847e3d06d735443eb3766ec2c5d08c33","value":777}},"04080a985fa34db29af8994f6a0bc194":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b91e3640d2ba43ea9f05a9d0effb5e97","placeholder":"​","style":"IPY_MODEL_6c122c2b2d9540f194738e19c26b8829","value":" 777/777 [00:00&lt;00:00, 68.2kB/s]"}},"0475fb1047c8432e8390fe5313583a94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d01e9a266b440cb95424dcee3ed86cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62e33de9223c4ea1b8da81e211a359eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"573e2e0409d340a68134f7f9039ca982":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"847e3d06d735443eb3766ec2c5d08c33":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b91e3640d2ba43ea9f05a9d0effb5e97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c122c2b2d9540f194738e19c26b8829":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9256d9eff364f7db99f28071beaec8f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_675920c17bfa47eab84156c93e3233c2","IPY_MODEL_fcf879b0ffa24d9e9af715fefbaddb30","IPY_MODEL_74f5d441552842e19b2f57ee863353fb"],"layout":"IPY_MODEL_59189e86ae8847ce8a9188d8b5f5f306"}},"675920c17bfa47eab84156c93e3233c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d92ad649906b4e5ca18b7f05bcdbb34a","placeholder":"​","style":"IPY_MODEL_8894308273814e949bf2d16aed220b7a","value":"model.safetensors: 100%"}},"fcf879b0ffa24d9e9af715fefbaddb30":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_459aa63b5d194fb2876f42ad75193596","max":437955512,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f4ade787d05442fb5eff0c0313823d6","value":437955512}},"74f5d441552842e19b2f57ee863353fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e7e2055d44c4c7095842d2dda9b4d70","placeholder":"​","style":"IPY_MODEL_27887d02f20b4af387015038c80227cd","value":" 438M/438M [00:01&lt;00:00, 347MB/s]"}},"59189e86ae8847ce8a9188d8b5f5f306":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d92ad649906b4e5ca18b7f05bcdbb34a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8894308273814e949bf2d16aed220b7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"459aa63b5d194fb2876f42ad75193596":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f4ade787d05442fb5eff0c0313823d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e7e2055d44c4c7095842d2dda9b4d70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27887d02f20b4af387015038c80227cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa289955cbab4a2b8b5ec4c4d067a9b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a90e4d363d24442a98d380d6631eda38","IPY_MODEL_85e940703c614b749d03bb44056d02fd","IPY_MODEL_fa5997b87c2d401ca9f69b28c5d7df87"],"layout":"IPY_MODEL_8bc21f36cedd4fa7bc6f512cf8ffa3a0"}},"a90e4d363d24442a98d380d6631eda38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86686f9edc7c4c5c879679eb627932b4","placeholder":"​","style":"IPY_MODEL_de00049777574399925d8e71b70cabc3","value":"tokenizer_config.json: 100%"}},"85e940703c614b749d03bb44056d02fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1a0d8760baf4bcb8827dd34aa671da9","max":366,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ac13195f33f46e283ed2e28b5e72ebc","value":366}},"fa5997b87c2d401ca9f69b28c5d7df87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5677316e59a43d8aa06d361247d9885","placeholder":"​","style":"IPY_MODEL_85476ce4f11042adbb52ee73d6fe9c69","value":" 366/366 [00:00&lt;00:00, 30.6kB/s]"}},"8bc21f36cedd4fa7bc6f512cf8ffa3a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86686f9edc7c4c5c879679eb627932b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de00049777574399925d8e71b70cabc3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1a0d8760baf4bcb8827dd34aa671da9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ac13195f33f46e283ed2e28b5e72ebc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a5677316e59a43d8aa06d361247d9885":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85476ce4f11042adbb52ee73d6fe9c69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebd1a35fb0244a08a32547a7d973f0ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e8a5681f3e494090b8baafbfd40f2a48","IPY_MODEL_68559088e7fb4570ac2c7d024aa55616","IPY_MODEL_5169342dfd984465870058f82c17caa6"],"layout":"IPY_MODEL_7d12f90c93f64aa98b35159be41df31d"}},"e8a5681f3e494090b8baafbfd40f2a48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50b68962b3cd4e1a88a609b88c70f07a","placeholder":"​","style":"IPY_MODEL_dc14a89bb78b49b0b7295b4049771ac9","value":"vocab.txt: 100%"}},"68559088e7fb4570ac2c7d024aa55616":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a305d29d982415b9a932f0d95049d7b","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a8639d5319244629e9355e99216870a","value":231508}},"5169342dfd984465870058f82c17caa6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4db51d77cc4b4004945958382a50210f","placeholder":"​","style":"IPY_MODEL_eb06afcaf0a048acb53215feb3b8152a","value":" 232k/232k [00:00&lt;00:00, 3.77MB/s]"}},"7d12f90c93f64aa98b35159be41df31d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50b68962b3cd4e1a88a609b88c70f07a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc14a89bb78b49b0b7295b4049771ac9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a305d29d982415b9a932f0d95049d7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a8639d5319244629e9355e99216870a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4db51d77cc4b4004945958382a50210f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb06afcaf0a048acb53215feb3b8152a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bb99b9b8c2646e7a2ae77bc17c46786":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e56ebfae77ea446f9ec831df0948f28a","IPY_MODEL_e0ed8ea4b1814a84aa3611a61d546a12","IPY_MODEL_d6888c5e6ba2462582e2d0509fe953e5"],"layout":"IPY_MODEL_d708d2bface9447e87da397af97dc636"}},"e56ebfae77ea446f9ec831df0948f28a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f08dcdd36b34479ab38fdf41b5e1e07c","placeholder":"​","style":"IPY_MODEL_ce9302b49e7a4ee1bdcacef65905c336","value":"tokenizer.json: 100%"}},"e0ed8ea4b1814a84aa3611a61d546a12":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d246c45da114a56b2f100c09ac8de33","max":711396,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f6f3d5df732b4794986646e5316e2be1","value":711396}},"d6888c5e6ba2462582e2d0509fe953e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeed601a7a40419296cca362b590199c","placeholder":"​","style":"IPY_MODEL_40b32373146f44e4a5603fba2f80e37d","value":" 711k/711k [00:00&lt;00:00, 4.17MB/s]"}},"d708d2bface9447e87da397af97dc636":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f08dcdd36b34479ab38fdf41b5e1e07c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce9302b49e7a4ee1bdcacef65905c336":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d246c45da114a56b2f100c09ac8de33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6f3d5df732b4794986646e5316e2be1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eeed601a7a40419296cca362b590199c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40b32373146f44e4a5603fba2f80e37d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80a7b39e038c428993c73265fece8618":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3b89c40955e47efb0565c2e1b85f351","IPY_MODEL_bf426203b4e14ef9b9489745c74a2f58","IPY_MODEL_444f60b2d11f462e91233cc502fc015b"],"layout":"IPY_MODEL_23001c77a6f247beb4f1546bbec6bd91"}},"a3b89c40955e47efb0565c2e1b85f351":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8be0ac1805f4547ad692f72cdb67051","placeholder":"​","style":"IPY_MODEL_cf393ce8e37046f380d26b27b131e1bb","value":"special_tokens_map.json: 100%"}},"bf426203b4e14ef9b9489745c74a2f58":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9495edc23934e3c8be307e95c66c629","max":125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e4822bc267e430780ef8f8262063df5","value":125}},"444f60b2d11f462e91233cc502fc015b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1845eb536b3491d9824cf20828cd842","placeholder":"​","style":"IPY_MODEL_187f09c8b1a142908c698d97325f4a77","value":" 125/125 [00:00&lt;00:00, 11.8kB/s]"}},"23001c77a6f247beb4f1546bbec6bd91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8be0ac1805f4547ad692f72cdb67051":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf393ce8e37046f380d26b27b131e1bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9495edc23934e3c8be307e95c66c629":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e4822bc267e430780ef8f8262063df5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1845eb536b3491d9824cf20828cd842":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"187f09c8b1a142908c698d97325f4a77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30509761bd574ad3a8f1591d12ffff56":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_447c8d3d6c4644d0a16cc721aaeff4d3","IPY_MODEL_0bec6eebe15e43498aad7f81b8b8035b","IPY_MODEL_a6e5a6c48fab4d008f19169caf3b94d7"],"layout":"IPY_MODEL_b2263caa7c5a42a481fe81797be054f2"}},"447c8d3d6c4644d0a16cc721aaeff4d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a05ee481e54d41eb9fe275f94bcee303","placeholder":"​","style":"IPY_MODEL_eb610cc546dc43eb81b32a2894183c8f","value":"1_Pooling/config.json: 100%"}},"0bec6eebe15e43498aad7f81b8b8035b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d3219cfb79d48ff958a05ce55cf6dbb","max":190,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac4549aaccbe4ca3954c2e72087f715b","value":190}},"a6e5a6c48fab4d008f19169caf3b94d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7939b8293bb94ddcb795462e138b605d","placeholder":"​","style":"IPY_MODEL_18ff9af2289e49418f3e557f7fe19ac6","value":" 190/190 [00:00&lt;00:00, 16.2kB/s]"}},"b2263caa7c5a42a481fe81797be054f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a05ee481e54d41eb9fe275f94bcee303":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb610cc546dc43eb81b32a2894183c8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d3219cfb79d48ff958a05ce55cf6dbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac4549aaccbe4ca3954c2e72087f715b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7939b8293bb94ddcb795462e138b605d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18ff9af2289e49418f3e557f7fe19ac6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}